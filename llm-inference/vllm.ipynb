{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Engine: vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows **how to run and measure interactive inference** with a local, fully-offline copy of *Mistral-Small-24B-Instruct* using the **vLLM** engine.  \n",
    "We will:\n",
    "\n",
    "1. set up a reproducible, internet-free environment;  \n",
    "2. load the 4-bit model and define a concise *system* instruction that keeps replies short and direct;  \n",
    "3. generate sample answers for two prompts to confirm everything is wired correctly;  \n",
    "4. implement a lightweight benchmark that reports average latency and output-tokens-per-second;  \n",
    "5. (optional) demonstrate token-by-token streaming with the KV-cache so you can watch the reply appear live.\n",
    "\n",
    "Feel free to tweak the prompts, sampling parameters, or `num_runs` variable to explore how temperature, max token count, and batch size affect throughput on your own hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2022.05-el8-x86_64/envs/vllm_serving/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 19:30:13 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 19:30:15,521\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os, time, torch\n",
    "from vllm import LLM, SamplingParams               \n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"                 # no outbound traffic\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant. You will be given a task and you should \"\n",
    "    \"respond with a solution. You should be concise and clear. One plain \"\n",
    "    \"paragraph—no lists, no headings, no filler.\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"Give me a short introduction to large language model inference.\",\n",
    "    \"The benefits of artificial intelligence in healthcare include:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 19:30:22 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-03 19:30:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 05-03 19:30:23 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501', speculative_config=None, tokenizer='/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-03 19:30:24 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0e554416c0>\n",
      "INFO 05-03 19:30:25 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-03 19:30:25 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-03 19:30:25 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-03 19:30:25 [gpu_model_runner.py:1329] Starting to load model /project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:02<00:18,  2.10s/it]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:03<00:15,  1.96s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:05<00:13,  1.86s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:07<00:10,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:09<00:08,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:11<00:07,  1.89s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:13<00:05,  1.88s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:14<00:03,  1.85s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:17<00:01,  1.94s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:19<00:00,  2.04s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:19<00:00,  1.93s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-03 19:30:44 [loader.py:458] Loading weights took 19.54 seconds\n",
      "INFO 05-03 19:30:45 [gpu_model_runner.py:1347] Model loading took 43.9150 GiB and 19.798823 seconds\n",
      "INFO 05-03 19:30:53 [backends.py:420] Using cache directory: /home/youzhi/.cache/vllm/torch_compile_cache/3d8394c036/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-03 19:30:53 [backends.py:430] Dynamo bytecode transform time: 8.44 s\n",
      "INFO 05-03 19:31:00 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.094 s\n",
      "INFO 05-03 19:31:01 [monitor.py:33] torch.compile takes 8.44 s in total\n",
      "INFO 05-03 19:31:08 [kv_cache_utils.py:634] GPU KV cache size: 223,952 tokens\n",
      "INFO 05-03 19:31:08 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 6.83x\n",
      "INFO 05-03 19:31:36 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 2.12 GiB\n",
      "INFO 05-03 19:31:36 [core.py:159] init engine (profile, create kv cache, warmup model) took 51.30 seconds\n",
      "INFO 05-03 19:31:36 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "sampling = SamplingParams(max_tokens=3096, temperature=0.7)   \n",
    "\n",
    "llm = LLM(model=model_path)                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                   | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████| 2/2 [00:07<00:00,  3.79s/it, est. speed input: 13.73 toks/s, output: 43.43 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Large language model inference refers to the process of generating outputs, such as text, from a pre-trained large language model (LLM) based on given inputs. During inference, the model takes an\n",
       "input sequence (such as a sentence or a prompt) and uses its learned patterns and structures from the training data to produce a coherent and contextually relevant continuation or response. This\n",
       "involves using the model's parameters to predict the next token (word or subword) in the sequence iteratively, often employing techniques like beam search or sampling to generate diverse and context-\n",
       "aware outputs. The quality of the generated text depends on the model's architecture, training data, and the specific inference techniques used."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "improved diagnosis accuracy, increased efficiency, personalized treatment plans, enhanced patient monitoring, and better data management. Provide a few examples of how these benefits can be realized.\n",
       "AI can be utilized to analyze vast amounts of medical data quickly and accurately, leading to improved diagnosis, for example, IBM's Watson for Oncology assists doctors in cancer treatment by sifting\n",
       "through patient data and the latest medical journals to recommend personalized treatment plans. In terms of efficiency, AI-driven chatbots can handle patient inquiries and administrative tasks,\n",
       "freeing up healthcare professionals to focus on patient care. Personalized treatment plans can be enhanced through AI algorithms that predict patient responses to different treatments, such as those\n",
       "used in precision medicine for tailoring cancer therapies. Patient monitoring can be improved with wearable devices and AI that continuously track vital signs and alert healthcare providers to any\n",
       "anomalies. Lastly, AI can manage and analyze large datasets to identify trends and patterns, enabling better healthcare resource allocation and policy-making."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "\n",
    "# generate answers and display just the wrapped assistant text\n",
    "for out in llm.generate([f\"{system_prompt}\\n{p}\" for p in prompts], sampling):\n",
    "    reply = textwrap.fill(out.outputs[0].text.strip(), width=200)\n",
    "    display(Markdown(reply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_benchmark(engine, sys_prompt, user_prompts,\n",
    "                   sampler, num_runs=3):\n",
    "    \"\"\"Return average latency (s) and output-tokens/s for a list of prompts.\"\"\"\n",
    "    # --- warm-up --------------------------------------------------------------\n",
    "    _ = list(engine.generate([f\"{sys_prompt}\\n{p}\" for p in user_prompts],\n",
    "                             sampler))\n",
    "    # --- timed runs -----------------------------------------------------------\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        _ = list(engine.generate([f\"{sys_prompt}\\n{p}\" for p in user_prompts],\n",
    "                                 sampler))\n",
    "        times.append(time.time() - start)\n",
    "    avg = sum(times) / len(times)\n",
    "    return {\n",
    "        \"avg_time_s\": avg,\n",
    "        \"tokens_per_second\": sampler.max_tokens / avg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                   | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████| 2/2 [00:08<00:00,  4.10s/it, est. speed input: 12.68 toks/s, output: 38.27 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████| 2/2 [00:05<00:00,  2.51s/it, est. speed input: 20.76 toks/s, output: 43.51 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████| 2/2 [00:05<00:00,  2.95s/it, est. speed input: 17.63 toks/s, output: 42.37 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████| 2/2 [00:05<00:00,  2.69s/it, est. speed input: 19.36 toks/s, output: 46.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 569.93 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = vllm_benchmark(llm, system_prompt, prompts, sampling, num_runs=3)\n",
    "\n",
    "print(f\"Throughput: {stats['tokens_per_second']:.2f} tokens/s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vLLM",
   "language": "python",
   "name": "vllm_serving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Engine: vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows **how to run and measure interactive inference** with a local, fully-offline copy of *Mistral-Small-24B-Instruct* using the **vLLM** engine.  \n",
    "We will:\n",
    "\n",
    "1. set up a reproducible, internet-free environment;  \n",
    "2. load the model in bfloat16 and define a concise *system* instruction that keeps replies short and direct;  \n",
    "3. generate sample answers for two prompts to confirm everything is wired correctly;  \n",
    "4. implement a lightweight benchmark that reports average latency and output-tokens-per-second;  \n",
    "\n",
    "Feel free to tweak the prompts, sampling parameters, or `num_runs` variable to explore how temperature, max token count, and batch size affect throughput on your own hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  7 12:32:03 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                On  | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              58W / 400W |      0MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2022.05-el8-x86_64/envs/vllm_serving/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 12:32:07 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 12:32:08,912\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os, time, torch\n",
    "from vllm import LLM, SamplingParams               \n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"                 # no outbound traffic\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant. You will be given a task and you should \"\n",
    "    \"respond with a solution. You should be concise and clear. One plain \"\n",
    "    \"paragraph—no lists, no headings, no filler.\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"Give me a short introduction to large language model inference.\",\n",
    "    \"The benefits of artificial intelligence in healthcare include:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 12:32:15 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 05-07 12:32:15 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 05-07 12:32:17 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501', speculative_config=None, tokenizer='/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-07 12:32:17 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff5067e5540>\n",
      "INFO 05-07 12:32:18 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-07 12:32:18 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 05-07 12:32:18 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 05-07 12:32:18 [gpu_model_runner.py:1329] Starting to load model /project/rcc/shared/ai-workshops/Mistral-Small-24B-Instruct-2501...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:01<00:17,  1.90s/it]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:03<00:14,  1.87s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:05<00:13,  1.91s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:07<00:10,  1.80s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:09<00:09,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:11<00:07,  1.83s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:12<00:05,  1.84s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:14<00:03,  1.87s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:16<00:01,  1.87s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:18<00:00,  1.86s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:18<00:00,  1.86s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 12:32:37 [loader.py:458] Loading weights took 18.66 seconds\n",
      "INFO 05-07 12:32:37 [gpu_model_runner.py:1347] Model loading took 43.9150 GiB and 19.159980 seconds\n",
      "INFO 05-07 12:32:53 [backends.py:420] Using cache directory: /home/youzhi/.cache/vllm/torch_compile_cache/3d8394c036/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-07 12:32:53 [backends.py:430] Dynamo bytecode transform time: 15.39 s\n",
      "INFO 05-07 12:32:59 [backends.py:136] Cache the graph of shape None for later use\n",
      "INFO 05-07 12:33:28 [backends.py:148] Compiling a graph for general shape takes 34.75 s\n",
      "INFO 05-07 12:33:52 [monitor.py:33] torch.compile takes 50.14 s in total\n",
      "INFO 05-07 12:33:54 [kv_cache_utils.py:634] GPU KV cache size: 223,936 tokens\n",
      "INFO 05-07 12:33:54 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 6.83x\n",
      "INFO 05-07 12:34:26 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 2.11 GiB\n",
      "INFO 05-07 12:34:27 [core.py:159] init engine (profile, create kv cache, warmup model) took 109.38 seconds\n",
      "INFO 05-07 12:34:27 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "sampling = SamplingParams(max_tokens=1024, temperature=0.7)   \n",
    "\n",
    "llm = LLM(model=model_path)                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.69s/it, est. speed input: 14.09 toks/s, output: 39.82 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Inference in large language models involves using a trained model to generate predictions or outputs based on new, unseen inputs. This process leverages the model's learned patterns and structures\n",
       "from the training data to produce coherent and contextually relevant responses. During inference, the model takes an input sequence (such as text) and processes it through its layers to generate an\n",
       "output sequence, which can be a continuation of the input or a completely new piece of text. The model's performance in inference is crucial for applications like natural language processing tasks,\n",
       "where the quality and relevance of the generated outputs are paramount."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "improved diagnostic accuracy, personalization of treatment plans, predictive analytics, and efficient management of patient data. However, there are also concerns about data privacy, the potential\n",
       "loss of jobs due to automation, and the ethical implications of AI decision-making. Additionally, there is uncertainty surrounding the regulation and governance of AI in healthcare. How can healthcare\n",
       "organizations address these concerns and challenges?  Healthcare organizations can address these concerns and challenges by implementing robust data governance and security protocols to protect\n",
       "patient privacy, ensuring transparency in AI algorithms to maintain public trust, and investing in workforce training to mitigate job displacement. They should also engage in ethical AI practices,\n",
       "such as bias mitigation and fair decision-making, and collaborate with regulatory bodies to develop clear guidelines for AI use. Moreover, fostering a culture of continuous learning and adaptation\n",
       "will help healthcare professionals stay updated with AI advancements, ensuring responsible and effective AI integration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import textwrap\n",
    "\n",
    "# generate answers and display just the wrapped assistant text\n",
    "for out in llm.generate([f\"{system_prompt}\\n{p}\" for p in prompts], sampling):\n",
    "    reply = textwrap.fill(out.outputs[0].text.strip(), width=200)\n",
    "    display(Markdown(reply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_benchmark(engine, sys_prompt, user_prompts,\n",
    "                   sampler, num_runs=3):\n",
    "    \"\"\"Return average latency (s) and output-tokens/s for a list of prompts.\"\"\"\n",
    "    # --- warm-up --------------------------------------------------------------\n",
    "    _ = list(engine.generate([f\"{sys_prompt}\\n{p}\" for p in user_prompts],\n",
    "                             sampler))\n",
    "    # --- timed runs -----------------------------------------------------------\n",
    "    times = []\n",
    "    token_counts = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        outputs = list(engine.generate([f\"{sys_prompt}\\n{p}\" for p in user_prompts],\n",
    "                                     sampler))\n",
    "        times.append(time.time() - start)\n",
    "        # Count actual tokens generated\n",
    "        total_output_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "        token_counts.append(total_output_tokens)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_tokens = sum(token_counts) / len(token_counts)\n",
    "    \n",
    "    return {\n",
    "        \"avg_time_s\": avg_time,\n",
    "        \"tokens_per_second\": avg_tokens / avg_time,\n",
    "        \"actual_tokens_generated\": avg_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.20s/it, est. speed input: 12.37 toks/s, output: 39.25 toks/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.65s/it, est. speed input: 11.18 toks/s, output: 36.65 toks/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.58s/it, est. speed input: 20.13 toks/s, output: 44.90 toks/s]\n",
      "Processed prompts: 100%|██████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.88s/it, est. speed input: 18.07 toks/s, output: 42.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 40.36 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = vllm_benchmark(llm, system_prompt, prompts, sampling, num_runs=3)\n",
    "\n",
    "print(f\"Throughput: {stats['tokens_per_second']:.2f} tokens/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored vLLM, a high-performance library for LLM inference. Key takeaways include:\n",
    "\n",
    "- vLLM significantly accelerates inference through PagedAttention, continuous batching, and optimized CUDA kernels\n",
    "- The library provides a simple API while handling complex memory management behind the scenes\n",
    "- Model deployment can scale from single GPU setups to distributed multi-GPU environments\n",
    "- vLLM supports popular model families (LLaMA, Mistral, Mixtral, etc.) with quantization options\n",
    "- Performance gains are most noticeable in high-throughput serving scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vLLM",
   "language": "python",
   "name": "vllm_serving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

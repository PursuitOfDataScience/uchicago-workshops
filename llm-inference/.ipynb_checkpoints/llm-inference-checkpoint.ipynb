{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Workshop\n",
    "\n",
    "This notebook explores how to run inference with Large Language Models (LLMs) efficiently, demonstrating the progression from full precision to quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers torch accelerate bitsandbytes psutil pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models with Different Precision\n",
    "\n",
    "Let's explore different precision options for LLM inference, starting with higher precision (fp16/bf16) and then moving to quantized models (8-bit, 4-bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2022.05-el8-x86_64/envs/transformers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_device_properties() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m hf_logging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_device_properties() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, logging as hf_logging\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.get_device_properties(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    \"\"\"Return current GPU memory usage in MB (allocated), or None if no GPU is available.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_bytes = torch.cuda.memory_allocated()\n",
    "        return allocated_bytes / (1024 * 1024)\n",
    "\n",
    "def inference_benchmark(model, tokenizer, prompt, num_runs=3, max_new_tokens=100):\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.generate(inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    return {\n",
    "        \"avg_time\": sum(times) / len(times),\n",
    "        \"tokens_per_second\": max_new_tokens / (sum(times) / len(times))\n",
    "    }\n",
    "\n",
    "# Choose a model for demonstration\n",
    "#model_id = \"/project/rcc/youzhi/toxic-models/Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "#model_id = \"/project/rcc/youzhi/toxic-models/Qwen/Qwen2.5-14B-Instruct-1M\"\n",
    "model_id = \"/project/rcc/youzhi/toxic-models/mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Storage for benchmark results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FP16 Precision (Half Precision)\n",
    "\n",
    "Let's first load the model in FP16 precision, which is a good balance between accuracy and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with FP16 precision...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "# Load model with FP16 precision\n",
    "start_time = time.time()\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    #device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_fp16.to(device)\n",
    "\n",
    "load_time_fp16 = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_fp16:.2f} seconds\")\n",
    "fp16_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some inference with the FP16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FP16 model inference\n",
    "prompt = \"Explain how transformers work in machine learning, focusing on self-attention mechanisms:\"\n",
    "\n",
    "fp16_benchmark = inference_benchmark(model_fp16, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"FP16\",\n",
    "    \"Memory (MB)\": fp16_memory,\n",
    "    \"Avg Generation Time (s)\": fp16_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": fp16_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Generate sample output\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_fp16.device)\n",
    "outputs = model_fp16.generate(inputs[\"input_ids\"], max_new_tokens=150)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sample output from FP16 model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BF16 Precision (Brain Floating Point)\n",
    "\n",
    "BF16 has the same number of bits as FP16 but a different distribution of precision, often better for training numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del model_fp16\n",
    "torch.cuda.empty_cache()\n",
    "print_memory_usage()\n",
    "\n",
    "# Load model with BF16 precision\n",
    "print(\"Loading model with BF16 precision...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "start_time = time.time()\n",
    "model_bf16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model_bf16.to(device)\n",
    "\n",
    "load_time_bf16 = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_bf16:.2f} seconds\")\n",
    "bf16_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {bf16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BF16 model inference\n",
    "bf16_benchmark = inference_benchmark(model_bf16, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"BF16\",\n",
    "    \"Memory (MB)\": bf16_memory,\n",
    "    \"Avg Generation Time (s)\": bf16_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": bf16_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Free up memory\n",
    "del model_bf16\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 8-bit Quantization\n",
    "\n",
    "Now let's try 8-bit quantization, which significantly reduces memory usage with minimal impact on quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with 8-bit quantization...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "start_time = time.time()\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "\n",
    "load_time_8bit = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_8bit:.2f} seconds\")\n",
    "int8_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {int8_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8-bit model inference\n",
    "int8_benchmark = inference_benchmark(model_8bit, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"INT8\",\n",
    "    \"Memory (MB)\": int8_memory,\n",
    "    \"Avg Generation Time (s)\": int8_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": int8_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Free up memory\n",
    "del model_8bit\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 4-bit Quantization\n",
    "\n",
    "Finally, let's try 4-bit quantization, which offers the maximum memory savings but may have more quality impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with 4-bit quantization...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "# Configure 4-bit quantization options\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # normalized float 4 format\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model_4bit.to(device)\n",
    "\n",
    "load_time_4bit = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_4bit:.2f} seconds\")\n",
    "int4_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {int4_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4-bit model inference\n",
    "int4_benchmark = inference_benchmark(model_4bit, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"INT4\",\n",
    "    \"Memory (MB)\": int4_memory,\n",
    "    \"Avg Generation Time (s)\": int4_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": int4_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Generate sample output to compare quality\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "outputs = model_4bit.generate(inputs[\"input_ids\"], max_new_tokens=150)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sample output from 4-bit quantized model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with Flash Attention in FP16 precision...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "# Load model with FP16 precision\n",
    "start_time = time.time()\n",
    "\n",
    "model_flashattention = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "load_time_fp16 = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_fp16:.2f} seconds\")\n",
    "fp16fa_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FP16 flash attention model inference\n",
    "fp16fa_benchmark = inference_benchmark(model_flashattention, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"FP16-FA\",\n",
    "    \"Memory (MB)\": fp16fa_memory,\n",
    "    \"Avg Generation Time (s)\": fp16fa_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": fp16fa_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Generate sample output to compare quality\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_flashattention.device)\n",
    "outputs = model_flashattention.generate(inputs[\"input_ids\"], max_new_tokens=150)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sample output from 16-bit flash attention quantized model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with Flash Attention in FP16 precision...\")\n",
    "base_memory = print_memory_usage()\n",
    "\n",
    "# Load model with FP16 precision\n",
    "start_time = time.time()\n",
    "\n",
    "model_flashattention = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model_flashattention = torch.compile(model_flashattention)\n",
    "\n",
    "load_time_fp16 = time.time() - start_time\n",
    "\n",
    "print(f\"Model loaded in {load_time_fp16:.2f} seconds\")\n",
    "fp16fa_memory = print_memory_usage() - base_memory\n",
    "print(f\"Memory footprint: {fp16fa_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FP16 flash attention model inference with fused kernels\n",
    "fused_fp16fa_benchmark = inference_benchmark(model_flashattention, tokenizer, prompt)\n",
    "results.append({\n",
    "    \"Precision\": \"FP16-FA-Fused\",\n",
    "    \"Memory (MB)\": fp16fa_memory,\n",
    "    \"Avg Generation Time (s)\": fused_fp16fa_benchmark[\"avg_time\"],\n",
    "    \"Tokens/Second\": fused_fp16fa_benchmark[\"tokens_per_second\"]\n",
    "})\n",
    "\n",
    "# Generate sample output to compare quality\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_flashattention.device)\n",
    "outputs = model_flashattention.generate(inputs[\"input_ids\"], max_new_tokens=150)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Sample output from 16-bit flash attention model with fused kernels:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results\n",
    "\n",
    "Let's visualize and compare the results from different precision formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a color map for each unique precision\n",
    "unique_precisions = results_df[\"Precision\"].unique()\n",
    "color_map = {prec: f\"C{i}\" for i, prec in enumerate(unique_precisions)}\n",
    "\n",
    "# Build the color arrays for each bar\n",
    "colors_for_memory = [color_map[prec] for prec in results_df[\"Precision\"]]\n",
    "colors_for_speed = [color_map[prec] for prec in results_df[\"Precision\"]]\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df[\"Precision\"], results_df[\"Memory (MB)\"], color=colors_for_memory)\n",
    "plt.title(\"GPU Memory Usage by Precision\")\n",
    "plt.ylabel(\"Memory Usage (MB)\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Plot inference speed comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df[\"Precision\"], results_df[\"Tokens/Second\"], color=colors_for_speed)\n",
    "plt.title(\"Inference Speed by Precision\")\n",
    "plt.ylabel(\"Tokens per Second\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top_k, top_p, tempurature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_new_tokens=200, \n",
    "    do_sample=False\n",
    ")\n",
    "greedy_response = tokenizer.decode(greedy_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Greedy decoding (no sampling) ===\")\n",
    "print(greedy_response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_new_tokens=200, \n",
    "    do_sample=True,\n",
    "    temperature=1.5  \n",
    ")\n",
    "temp_response = tokenizer.decode(temp_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Temperature Sampling (T=1.5) ===\")\n",
    "print(temp_response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_new_tokens=200, \n",
    "    do_sample=True,\n",
    "    temperature=5.5  \n",
    ")\n",
    "temp_response = tokenizer.decode(temp_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Temperature Sampling (T=5.5) ===\")\n",
    "print(temp_response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_new_tokens=200, \n",
    "    do_sample=True,\n",
    "    temperature=0.7, \n",
    "    top_k=50  # Only sample from top 50 tokens\n",
    ")\n",
    "topk_response = tokenizer.decode(topk_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Top-k Sampling (k=50, T=0.7) ===\")\n",
    "print(topk_response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topp_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9   # Keep the top 90% probability mass\n",
    ")\n",
    "topp_response = tokenizer.decode(topp_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Top-p Sampling (p=0.9, T=0.7) ===\")\n",
    "print(topp_response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_outputs = model_4bit.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "combo_response = tokenizer.decode(combo_outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Combined Sampling (top_k=50, top_p=0.9, T=0.8) ===\")\n",
    "print(combo_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Inference Techniques\n",
    "\n",
    "Now that we've selected 4-bit quantization for its efficiency, let's explore more advanced inference techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing for Throughput\n",
    "\n",
    "Processing multiple inputs at once can be more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of prompts\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks\",\n",
    "    \"How do transformers work?\",\n",
    "    \"Define reinforcement learning\",\n",
    "    \"What is transfer learning?\"\n",
    "]\n",
    "\n",
    "# Process individually\n",
    "start_time = time.time()\n",
    "individual_outputs = []\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "    outputs = model_4bit.generate(inputs[\"input_ids\"], max_new_tokens=500)\n",
    "    individual_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "individual_time = time.time() - start_time\n",
    "print(f\"Individual processing time: {individual_time:.2f}s\")\n",
    "\n",
    "# Process as a batch\n",
    "start_time = time.time()\n",
    "batch_inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(model_4bit.device)\n",
    "batch_outputs = model_4bit.generate(batch_inputs[\"input_ids\"], max_new_tokens=500)\n",
    "batch_decoded = [tokenizer.decode(output, skip_special_tokens=True) for output in batch_outputs]\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"Batch processing time: {batch_time:.2f}s\")\n",
    "print(f\"Speed improvement: {individual_time/batch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using KV Caching for Faster Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the explicit use of KV cache for token generation\n",
    "\n",
    "prompt = \"The benefits of artificial intelligence in healthcare include:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "\n",
    "# disable KV cache explicitly\n",
    "start_time = time.time()\n",
    "outputs1 = model_4bit.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=50,\n",
    "    use_cache=False          # <- turn off the KV cache\n",
    ")\n",
    "time1 = time.time() - start_time\n",
    "print(f\"Standard generation time (noâ€‘cache): {time1:.2f}s\")\n",
    "\n",
    "\n",
    "# enable KV cache explicitly\n",
    "start_time = time.time()\n",
    "outputs1 = model_4bit.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=50,\n",
    "    use_cache=True          # <- turn on the KV cache\n",
    ")\n",
    "time2 = time.time() - start_time\n",
    "print(f\"KV cache generation time: {time2:.2f}s\")\n",
    "\n",
    "\n",
    "# Method 2: Manually implementing token-by-token generation with KV cache\n",
    "# start_time = time.time()\n",
    "# generated_ids = inputs.input_ids\n",
    "# past_key_values = None\n",
    "\n",
    "# for _ in range(50):  \n",
    "#     # Forward pass with caching\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model_4bit(\n",
    "#             input_ids=generated_ids[:, -1:] if past_key_values is not None else generated_ids,\n",
    "#             past_key_values=past_key_values,\n",
    "#             use_cache=True\n",
    "#         )\n",
    "    \n",
    "#     past_key_values = outputs.past_key_values  # Cache for next iteration\n",
    "#     next_token_id = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(-1)\n",
    "#     generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    \n",
    "#     # Check for EOS token\n",
    "#     if next_token_id.item() == tokenizer.eos_token_id:\n",
    "#         break\n",
    "\n",
    "# time2 = time.time() - start_time\n",
    "# print(f\"Explicit KV cache generation time: {time2:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Generation\n",
    "\n",
    "For better user experience, we can stream the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "prompt = \"Write a short introduction about machine learning:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "\n",
    "# Streamed generation with proper KV caching\n",
    "generated_ids = inputs.input_ids\n",
    "past_key_values = None\n",
    "response = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "for _ in range(200):  \n",
    "    clear_output(wait=True)\n",
    "    print(response, end=\"\")\n",
    "    \n",
    "    # Generate next token\n",
    "    with torch.no_grad():\n",
    "        outputs = model_4bit(\n",
    "            input_ids=generated_ids[:, -1:] if past_key_values is not None else generated_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # Sample next token (instead of greedy)\n",
    "    logits = outputs.logits[:, -1, :] / 0.7  # temperature = 0.7\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    # Decode and display\n",
    "    next_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "    response += next_token\n",
    "    \n",
    "    # Check for EOS token\n",
    "    if next_token_id.item() == tokenizer.eos_token_id:\n",
    "        break\n",
    "        \n",
    "    time.sleep(0.1)  # Slow down for visual effect\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative Decoding\n",
    "\n",
    "Let's implement a simple version of speculative decoding using a smaller model to predict tokens, then verify with our larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def speculative_decoding(\n",
    "    model_draft, \n",
    "    model_main, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    draft_tokens=4, \n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using speculative decoding, where:\n",
    "      - model_draft (smaller) proposes 'draft_tokens' at a time\n",
    "      - model_main (larger) verifies or rejects those tokens in a single pass\n",
    "\n",
    "    Args:\n",
    "        model_draft: The smaller \"draft\" model (e.g. 4-bit quantized).\n",
    "        model_main: The larger \"main\" model (e.g. FP16).\n",
    "        tokenizer: Tokenizer for both models (assume they share the vocab).\n",
    "        prompt: Initial text prompt (str).\n",
    "        draft_tokens: Number of tokens proposed by model_draft in each chunk.\n",
    "        max_new_tokens: Maximum tokens to generate (not counting the prompt).\n",
    "        temperature: Sampling temperature for the draft model (and main if it must sample).\n",
    "\n",
    "    Returns:\n",
    "        A string containing the generated text.\n",
    "    \"\"\"\n",
    "    # Move models to GPU if available (and set eval mode)\n",
    "    model_draft.to(device).eval()\n",
    "    model_main.to(device).eval()\n",
    "\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = inputs[\"input_ids\"]  # shape: (1, prompt_length)\n",
    "\n",
    "    accepted_tokens = 0\n",
    "    rejected_tokens = 0\n",
    "    prompt_len = generated_ids.shape[1]  # to track how many new tokens we generate\n",
    "\n",
    "    # We iterate until we produce max_new_tokens or hit EOS\n",
    "    while (generated_ids.shape[1] - prompt_len) < max_new_tokens:\n",
    "        # 1) Use the draft model to propose a chunk of 'draft_tokens'\n",
    "        #    We'll do a short generation with do_sample and some temperature.\n",
    "        #    This returns the entire sequence, so we'll slice out just the new part.\n",
    "        draft_out = model_draft.generate(\n",
    "            generated_ids,\n",
    "            max_new_tokens=draft_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )[0]\n",
    "        \n",
    "        # Extract only the newly proposed tokens (the \"chunk\")\n",
    "        proposed_chunk = draft_out[generated_ids.shape[1]:]  # shape: (chunk_len,)\n",
    "\n",
    "        # If no new tokens (model ended with EOS), break\n",
    "        if len(proposed_chunk) == 0:\n",
    "            break\n",
    "\n",
    "        # 2) Single forward pass in the main model to verify chunk\n",
    "        #    We'll do teacher forcing on the proposed_chunk to get probabilities.\n",
    "        #    That means we feed the main model all tokens up to, but not including,\n",
    "        #    each target token position.\n",
    "        #\n",
    "        #    Instead of forward passing N times, we do ONE pass over:\n",
    "        #       cat(generated_ids, proposed_chunk[:-1])\n",
    "        #    so the final logit will correspond to the last token in 'proposed_chunk'.\n",
    "        #\n",
    "        #    Then we'll compare each position's sample with the draft token.\n",
    "        # -------------------------------------------------------------------------\n",
    "        if len(proposed_chunk) > 1:\n",
    "            # main_input includes everything up to the last token of the chunk - 1\n",
    "            main_input = torch.cat([generated_ids, proposed_chunk[:-1].unsqueeze(0)], dim=1)\n",
    "        else:\n",
    "            # If there's only 1 token in the chunk, there's no \"chunk[:-1]\",\n",
    "            # so just use the existing generated_ids.\n",
    "            main_input = generated_ids\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs_main = model_main(main_input, use_cache=False)\n",
    "            # logits shape: (batch=1, seq_len, vocab_size)\n",
    "            logits_main = outputs_main.logits\n",
    "\n",
    "        # We'll collect the accepted tokens from this chunk\n",
    "        accepted_chunk = []\n",
    "        chunk_rejected = False\n",
    "\n",
    "        # Figure out how to index the logits for each new token\n",
    "        # If main_input has length L, the i-th new token's logit is at index L-1 + i\n",
    "        # because the model at position L-1 predicts the token at position L (the first chunk token), etc.\n",
    "        # The total new chunk length = len(proposed_chunk).\n",
    "        offset = logits_main.shape[1] - 1  # the last logit index in main_input\n",
    "\n",
    "        for i, draft_token in enumerate(proposed_chunk):\n",
    "            # 2a) Compare the main model's distribution to the draft token\n",
    "            # The logit index for this new token = offset + i\n",
    "            logit_idx = offset + i\n",
    "            # If that index is out of range, something went wrong\n",
    "            if logit_idx >= logits_main.shape[1]:\n",
    "                break\n",
    "\n",
    "            # Probability distribution over the vocab at this position\n",
    "            dist = F.softmax(logits_main[0, logit_idx, :], dim=-1)\n",
    "\n",
    "            # Sample from the main model's distribution\n",
    "            main_sample_id = torch.multinomial(dist, num_samples=1)\n",
    "\n",
    "            if main_sample_id.item() == draft_token.item():\n",
    "                # 2b) Accept the draft token\n",
    "                accepted_chunk.append(draft_token)\n",
    "                accepted_tokens += 1\n",
    "            else:\n",
    "                # 2c) Reject this token (and all subsequent tokens in chunk)\n",
    "                #     We'll fallback to the main model's token here,\n",
    "                #     and discard the rest of the chunk.\n",
    "                accepted_chunk.append(main_sample_id.squeeze())\n",
    "                rejected_tokens += (len(proposed_chunk) - i)\n",
    "                chunk_rejected = True\n",
    "                break\n",
    "\n",
    "        # Now we cat the accepted_chunk to the generated_ids\n",
    "        if len(accepted_chunk) > 0:\n",
    "            accepted_tensor = torch.stack(accepted_chunk).unsqueeze(0)  # shape: (1, n_accepted)\n",
    "            generated_ids = torch.cat([generated_ids, accepted_tensor], dim=1)\n",
    "\n",
    "        # If we rejected at some point, we stop verifying the remainder of the chunk.\n",
    "        # We'll continue generating from the next iteration.\n",
    "        # Check if we hit EOS\n",
    "        if generated_ids[0, -1].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Done generating or hit EOS\n",
    "    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    total_proposed = accepted_tokens + rejected_tokens\n",
    "    accept_rate = accepted_tokens / total_proposed if total_proposed > 0 else 1.0\n",
    "    \n",
    "    print(f\"Tokens accepted: {accepted_tokens}, rejected: {rejected_tokens}\")\n",
    "    print(f\"Acceptance rate: {accept_rate:.2%}\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "#result = speculative_decoding(model_4bit, model_fp16, tokenizer, prompt, draft_tokens=4, max_new_tokens=50)\n",
    "#print(\"Speculative Decoding Output:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Resources\n",
    "\n",
    "In this notebook, we've explored various techniques for efficient LLM inference:\n",
    "\n",
    "1. Different precision formats (FP16, BF16, INT8, INT4) with their memory and speed tradeoffs\n",
    "2. Batch processing for increased throughput\n",
    "3. KV caching for faster token generation\n",
    "4. Streaming generation for better user experience\n",
    "5. Speculative decoding to speed up inference\n",
    "\n",
    "For further optimization, consider exploring:\n",
    "\n",
    "- Specialized inference libraries like vLLM, DeepSpeed or TensorRT-LLM\n",
    "- Mixture of Experts (MoE) models like Mixtral\n",
    "- Model distillation to create smaller, faster models\n",
    "- Prompt engineering for token efficiency\n",
    "- Hardware-specific optimizations (CUDA graphs, FlashAttention, etc.)\n",
    "\n",
    "Key references:\n",
    "- [Hugging Face Efficient Inference Guide](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [Speculative Decoding Paper](https://arxiv.org/abs/2302.01318)\n",
    "- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Env",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

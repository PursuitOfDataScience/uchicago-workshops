{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Prompt Engineering  \n",
        "\n",
        "This hands-on tutorial will teach you advanced techniques for crafting effective prompts that maximize the performance of AI models like Llama-4-Maverick delivered by Meta's Llama API under research preview.\n",
        "\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "In this workshop, we'll explore five core prompt engineering techniques using real-world examples:\n",
        "\n",
        "1. **Zero-Shot Prompting**: Getting results without examples\n",
        "2. **Few-Shot Prompting**: Learning from examples\n",
        "3. **Chain of Thought Reasoning**: Step-by-step problem solving\n",
        "4. **Role Prompting**: Giving the AI a specific persona\n",
        "5. **Data Cleaning Applications**: Practical business use cases\n",
        "\n",
        "Each section includes detailed explanations, code examples, and hands-on exercises that you can run immediately.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup and Basic Usage\n",
        "\n",
        "Before diving into advanced techniques, let's establish our foundation. We'll set up our environment and create a robust helper function that will be the backbone of all our experiments.\n",
        "\n",
        "### Understanding the Llama API Structure\n",
        "\n",
        "The Llama API follows the OpenAI-style chat completion format, where conversations are represented as lists of messages. Each message has a role (like 'user', 'system', or 'assistant') and content. This structure allows for rich, contextual interactions that can maintain conversation history and role-specific behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Response: Hello! I'm Llama, a Meta-designed model here to adapt to your conversational style. Whether you need quick answers, deep dives into ideas, or just want to vent, joke or brainstorm—I'm here for it. What’s on your mind?\n"
          ]
        }
      ],
      "source": [
        "#!pip install llama-api-client\n",
        "import os\n",
        "from llama_api_client import LlamaAPIClient\n",
        "\n",
        "# Initialize the client\n",
        "client = LlamaAPIClient(\n",
        "    #api_key=os.environ.get(\"LLAMA_API_KEY\")\n",
        "    api_key=\"LLM|1465877834571093|6OzJ14guJ8G-LGcetXMwAx3kdnI\" \n",
        ")\n",
        "\n",
        "# Basic function to interact with the model\n",
        "def call_llama(messages, model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\"):\n",
        "    \"\"\"\n",
        "    Helper function to call Llama API with error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "        )\n",
        "        return completion.completion_message.content.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Test basic functionality\n",
        "response = call_llama([\n",
        "    {\"role\": \"user\", \"content\": \"Hello, can you introduce yourself?\"}\n",
        "])\n",
        "print(\"Basic Response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Breaking Down Our Helper Function\n",
        "\n",
        "The `call_llama` function we just created is more than a simple API wrapper—it's a robust foundation for prompt engineering:\n",
        "\n",
        "**Key Features:**\n",
        "- **Error Handling**: Catches API errors and network issues gracefully\n",
        "- **Flexible Messaging**: Accepts any message format (user, system, assistant roles)\n",
        "- **Model Selection**: Allows switching between different Llama models\n",
        "- **Consistent Interface**: Standardizes how we interact with the API throughout the workshop\n",
        "\n",
        "**Why This Matters:**\n",
        "In production environments, API calls can fail for various reasons (network issues, rate limits, model availability). Our helper function ensures that your prompt engineering experiments won't crash unexpectedly, making it easier to iterate and refine your techniques.\n",
        "\n",
        "The test call we make verifies that:\n",
        "1. Your API key is correctly configured\n",
        "2. The model is accessible\n",
        "3. The basic communication pipeline works\n",
        "\n",
        "---\n",
        "\n",
        "## Zero-Shot Prompting\n",
        "\n",
        "Zero-shot prompting is the foundation of prompt engineering. It's called \"zero-shot\" because you're asking the model to perform a task without providing any examples—you're giving it \"zero shots\" to learn from. The model relies entirely on its pre-training to understand and complete the task.\n",
        "\n",
        "### When to Use Zero-Shot Prompting\n",
        "\n",
        "Zero-shot prompting works best for:\n",
        "- **Common tasks**: Things the model has seen many times during training\n",
        "- **Well-defined problems**: Tasks with clear, unambiguous requirements\n",
        "- **Quick prototyping**: When you need fast results without setup time\n",
        "- **Simple classifications**: Basic categorization or labeling tasks\n",
        "\n",
        "### The Anatomy of a Good Zero-Shot Prompt\n",
        "\n",
        "Effective zero-shot prompts typically include:\n",
        "1. **Clear task description**: What you want the model to do\n",
        "2. **Input specification**: What data you're providing\n",
        "3. **Output format**: How you want the response structured\n",
        "4. **Context or constraints**: Any important limitations or requirements\n",
        "\n",
        "### Simple Zero-Shot Example: Sentiment Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot Classification Result:\n",
            "The classification of the given text is: **positive**.\n",
            "\n",
            "The text contains strong positive language, such as \"absolutely love\", \"amazing\", and \"excellent\", indicating a very favorable opinion of the restaurant.\n"
          ]
        }
      ],
      "source": [
        "def zero_shot_classification():\n",
        "    \"\"\"\n",
        "    Demonstrate zero-shot text classification\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \n",
        "            \"content\": \"\"\"Classify the following text as either 'positive', 'negative', or 'neutral':\n",
        "\n",
        "Text: \"I absolutely love this new restaurant! The food was amazing and the service was excellent.\"\n",
        "\n",
        "Classification:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Zero-shot Classification Result:\")\n",
        "    print(response)\n",
        "\n",
        "zero_shot_classification()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot Classification Result:\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "def zero_shot_classification_refined():\n",
        "    \"\"\"\n",
        "    Demonstrate zero-shot text classification\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\", \n",
        "            \"content\": \"\"\"Classify the following text as either 'positive', 'negative', or 'neutral'. Only output one word:\n",
        "\n",
        "Text: \"I absolutely love this new restaurant! The food was amazing and the service was excellent.\"\n",
        "\n",
        "Classification:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Zero-shot Classification Result:\")\n",
        "    print(response)\n",
        "\n",
        "zero_shot_classification_refined()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Why This Example Works**: The model has seen countless examples of sentiment analysis during training, so it can reliably identify emotional tone without additional guidance.\n",
        "\n",
        "### Zero-Shot Question Answering: Working with Context\n",
        "\n",
        "Question answering with context is a more complex zero-shot task that demonstrates the model's reading comprehension abilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot QA Result:\n",
            "Guido van Rossum in 1991.\n"
          ]
        }
      ],
      "source": [
        "def zero_shot_qa():\n",
        "    \"\"\"\n",
        "    Demonstrate zero-shot question answering with context\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Answer the following question based on the given context:\n",
        "\n",
        "Context: \"Python is a high-level programming language created by Guido van Rossum in 1991. It emphasizes code readability and simplicity, making it popular for beginners and experts alike. Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.\"\n",
        "\n",
        "Question: Who created Python and when?\n",
        "\n",
        "Answer:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Zero-shot QA Result:\")\n",
        "    print(response)\n",
        "\n",
        "zero_shot_qa()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Insight**: Zero-shot QA works well when the answer is explicitly stated in the context. For more complex reasoning or implicit information, you might need few-shot examples or chain of thought prompting.\n",
        "\n",
        "### Zero-Shot Limitations and Solutions\n",
        "\n",
        "While powerful, zero-shot prompting has limitations:\n",
        "- **Inconsistent formatting**: Without examples, output format may vary\n",
        "- **Ambiguous tasks**: Complex or unusual tasks may be misinterpreted\n",
        "- **Domain-specific knowledge**: Specialized fields may require more guidance\n",
        "\n",
        "When zero-shot prompting isn't sufficient, that's when few-shot prompting becomes valuable.\n",
        "\n",
        "---\n",
        "\n",
        "## Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting is like teaching by example. Instead of just describing what you want, you show the model a few examples of the input-output pattern you're looking for. This technique dramatically improves performance on tasks where zero-shot prompting produces inconsistent or suboptimal results.\n",
        "\n",
        "### The Psychology of Few-Shot Learning\n",
        "\n",
        "Few-shot prompting works because it:\n",
        "- **Establishes patterns**: Shows the model exactly what \"good\" looks like\n",
        "- **Reduces ambiguity**: Eliminates guesswork about output format\n",
        "- **Provides context**: Helps the model understand edge cases and nuances\n",
        "- **Enables consistency**: Creates a template for reliable, standardized responses\n",
        "\n",
        "### How Many Examples Should You Use?\n",
        "\n",
        "The \"few\" in few-shot typically means:\n",
        "- **2-3 examples**: Often sufficient for simple tasks\n",
        "- **3-5 examples**: Good for more complex patterns\n",
        "- **5+ examples**: Rarely needed, may hit token limits\n",
        "\n",
        "**Quality over quantity**: Three diverse, high-quality examples usually outperform ten similar ones.\n",
        "\n",
        "### Few-Shot Classification: Building on Zero-Shot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot Classification Result:\n",
            "The classification of the text: \"The customer service was disappointing and unhelpful.\" is: negative.\n",
            "\n",
            "The text expresses a clear negative sentiment towards the customer service, using words like \"disappointing\" and \"unhelpful\" to convey a unfavorable opinion.\n"
          ]
        }
      ],
      "source": [
        "def few_shot_classification():\n",
        "    \"\"\"\n",
        "    Demonstrate few-shot learning for sentiment classification\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Classify the following texts as 'positive', 'negative', or 'neutral':\n",
        "\n",
        "Example 1:\n",
        "Text: \"This movie was terrible, I want my money back.\"\n",
        "Classification: negative\n",
        "\n",
        "Example 2:\n",
        "Text: \"The weather is okay today, not too hot or cold.\"\n",
        "Classification: neutral\n",
        "\n",
        "Example 3:\n",
        "Text: \"I'm so excited about my vacation next week!\"\n",
        "Classification: positive\n",
        "\n",
        "Now classify this text:\n",
        "Text: \"The customer service was disappointing and unhelpful.\"\n",
        "Classification:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Few-shot Classification Result:\")\n",
        "    print(response)\n",
        "    \n",
        "few_shot_classification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-Shot Data Extraction: Structured Output\n",
        "\n",
        "Data extraction tasks particularly benefit from few-shot prompting because they require consistent, structured output formats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot Extraction Result:\n",
            "Based on the provided text, the extraction is:\n",
            "\n",
            "{\"name\": \"Emma Davis\", \"age\": 32, \"occupation\": \"marketing manager\"}\n"
          ]
        }
      ],
      "source": [
        "def few_shot_extraction():\n",
        "    \"\"\"\n",
        "    Demonstrate few-shot learning for structured data extraction\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Extract name, age, and occupation from the following texts:\n",
        "\n",
        "Example 1:\n",
        "Text: \"Hi, I'm Sarah Johnson, a 28-year-old software engineer.\"\n",
        "Extraction: {\"name\": \"Sarah Johnson\", \"age\": 28, \"occupation\": \"software engineer\"}\n",
        "\n",
        "Example 2:\n",
        "Text: \"My name is Michael Chen and I'm 35. I work as a doctor.\"\n",
        "Extraction: {\"name\": \"Michael Chen\", \"age\": 35, \"occupation\": \"doctor\"}\n",
        "\n",
        "Now extract from this text:\n",
        "Text: \"Hello, I'm Emma Davis, I'm 32 years old and I'm a marketing manager.\"\n",
        "Extraction:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Few-shot Extraction Result:\")\n",
        "    print(response)\n",
        "\n",
        "few_shot_extraction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Practices for Few-Shot Example Selection\n",
        "\n",
        "When choosing examples for few-shot prompting:\n",
        "\n",
        "**Diversity is crucial**: \n",
        "- Different input formats and styles\n",
        "- Various edge cases and scenarios\n",
        "- Representative of real-world data variation\n",
        "\n",
        "**Quality matters**: \n",
        "- Perfect examples teach perfect patterns\n",
        "- Consistent formatting across all examples\n",
        "- Clear, unambiguous input-output relationships\n",
        "\n",
        "**Strategic coverage**: \n",
        "- Include edge cases you expect to encounter\n",
        "- Show how to handle missing or unclear information\n",
        "- Demonstrate the desired level of detail\n",
        "\n",
        "---\n",
        "\n",
        "## Chain of Thought Reasoning\n",
        "\n",
        "Chain of Thought (CoT) prompting is a revolutionary technique that encourages models to \"think out loud\" by showing their reasoning process step by step. This approach is particularly powerful for complex problems that require multi-step reasoning, mathematical calculations, or logical analysis.\n",
        "\n",
        "### Why Chain of Thought Works\n",
        "\n",
        "Traditional prompting often produces answers without explanation, like a student showing only the final answer on a math test. CoT prompting is like asking the student to \"show their work\"—and remarkably, this improves accuracy even when the model isn't specifically trained for reasoning.\n",
        "\n",
        "**The Science Behind CoT:**\n",
        "- **Decomposition**: Complex problems become manageable sub-problems\n",
        "- **Error reduction**: Step-by-step reasoning catches logical errors\n",
        "- **Transparency**: You can see where and why the model might go wrong\n",
        "- **Confidence building**: Explicit reasoning builds trust in the output\n",
        "\n",
        "### CoT with Non-Reasoning Models\n",
        "\n",
        "While Llama 4 Maverick isn't specifically designed as a reasoning model (like GPT-O3 or Gemini 2.5 Pro), we can still elicit step-by-step thinking through careful prompt engineering. The key is creating a framework that encourages methodical problem-solving.\n",
        "\n",
        "### Mathematical Problem Solving with CoT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain of Thought Math Result:\n",
            "To solve the problem, let's follow the steps you've outlined.\n",
            "\n",
            "**Step 1: Find the total number of apples sold**\n",
            "\n",
            "The store sells 18 apples in the morning and 12 apples in the afternoon. To find the total number of apples sold, we need to add these two numbers together.\n",
            "\n",
            "Total apples sold = Apples sold in the morning + Apples sold in the afternoon\n",
            "Total apples sold = 18 + 12\n",
            "Total apples sold = 30\n",
            "\n",
            "**Step 2: Subtract the total number of apples sold from the original amount**\n",
            "\n",
            "The store originally had 45 apples. We now know that they sold 30 apples in total. To find out how many apples are left, we need to subtract the total number of apples sold from the original amount.\n",
            "\n",
            "Apples left = Original number of apples - Total apples sold\n",
            "Apples left = 45 - 30\n",
            "Apples left = 15\n",
            "\n",
            "Therefore, there are **15 apples left** in the store.\n"
          ]
        }
      ],
      "source": [
        "def chain_of_thought_math():\n",
        "    \"\"\"\n",
        "    Demonstrate chain of thought reasoning for math problems\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Solve this step by step, showing your reasoning:\n",
        "\n",
        "Problem: A store has 45 apples. They sell 18 apples in the morning and 12 apples in the afternoon. How many apples are left?\n",
        "\n",
        "Let me think through this step by step:\n",
        "1) First, I need to find the total number of apples sold\n",
        "2) Then subtract that from the original amount\n",
        "\n",
        "Step-by-step solution:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Chain of Thought Math Result:\")\n",
        "    print(response)\n",
        "\n",
        "chain_of_thought_math()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logical Reasoning with CoT\n",
        "\n",
        "CoT is particularly valuable for logical puzzles and philosophical problems where the reasoning process is as important as the conclusion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain of Thought Logic Result:\n",
            "## Step 1: Identify the given statements\n",
            "The given statements are: \"All birds can fly\" and \"Penguins are birds.\" These are the premises of the argument.\n",
            "\n",
            "## Step 2: Apply logical reasoning\n",
            "To determine if penguins can fly, we need to apply the given premises to the question. The first premise states that all birds can fly, and the second premise states that penguins are birds. Using logical deduction, if penguins are classified as birds and all birds can fly, then it logically follows that penguins can fly according to the given premises.\n",
            "\n",
            "## Step 3: Evaluate the conclusion based on the premises\n",
            "However, we must also consider the real-world truth and the validity of the premises. The statement \"All birds can fly\" is not true in reality because there are birds, like penguins and ostriches, that cannot fly. Despite this, within the context of the given problem, we are to assume the premises are true.\n",
            "\n",
            "## Step 4: Provide the conclusion with explanation\n",
            "Based on the given premises, \"All birds can fly\" and \"Penguins are birds,\" we can logically conclude that penguins can fly. This conclusion follows the rules of deductive reasoning, where if the premises are true, the conclusion must also be true. However, it's crucial to note that the first premise is factually incorrect in the real world. Penguins are birds, but they cannot fly. The conclusion that penguins can fly is a result of applying the given logical structure, not a reflection of real-world facts.\n",
            "\n",
            "## Step 5: Consider the real-world implication\n",
            "In reality, we know penguins are birds but cannot fly, which makes the initial premise \"All birds can fly\" false. This highlights the importance of premise truth in logical reasoning. Even though the logical structure is valid (if all birds can fly and penguins are birds, then penguins can fly), the conclusion is not true because one of the premises is false.\n",
            "\n",
            "The final answer is: $\\boxed{Yes}$\n"
          ]
        }
      ],
      "source": [
        "def chain_of_thought_logic():\n",
        "    \"\"\"\n",
        "    Demonstrate chain of thought for logical reasoning\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Analyze this logical problem step by step:\n",
        "\n",
        "Problem: All birds can fly. Penguins are birds. Can penguins fly?\n",
        "\n",
        "Please think through this carefully:\n",
        "1) First, identify the given statements\n",
        "2) Then apply logical reasoning\n",
        "3) Finally, provide your conclusion with explanation\n",
        "\n",
        "Analysis:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Chain of Thought Logic Result:\")\n",
        "    print(response)\n",
        "\n",
        "chain_of_thought_logic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complex Multi-Step CoT Reasoning\n",
        "\n",
        "For more sophisticated problems, we can create detailed reasoning frameworks that guide the model through complex calculations and decisions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complex Chain of Thought Result:\n",
            "To solve this problem, we'll break it down into the required steps.\n",
            "\n",
            "### Step 1: Calculate current distribution\n",
            "\n",
            "First, let's determine how many employees are currently working in sales, engineering, and administration.\n",
            "\n",
            "- Total employees = 100\n",
            "- Employees in sales = 60% of 100 = 0.6 * 100 = 60\n",
            "- Employees in engineering = 25% of 100 = 0.25 * 100 = 25\n",
            "- Employees in administration = 100% - (60% + 25%) = 15% of 100 = 0.15 * 100 = 15\n",
            "\n",
            "So, currently, there are 60 employees in sales, 25 in engineering, and 15 in administration.\n",
            "\n",
            "### Step 2: Calculate new hires impact\n",
            "\n",
            "The company is hiring 20 new salespeople and 10 new engineers.\n",
            "\n",
            "- New employees in sales = 60 (current) + 20 (new) = 80\n",
            "- New employees in engineering = 25 (current) + 10 (new) = 35\n",
            "- Employees in administration remain the same = 15\n",
            "\n",
            "### Step 3: Calculate new total and sales percentage\n",
            "\n",
            "Now, let's calculate the new total number of employees and the percentage of employees in sales.\n",
            "\n",
            "- New total employees = Original 100 employees + 20 new salespeople + 10 new engineers = 100 + 20 + 10 = 130\n",
            "- New total employees in sales = 80\n",
            "\n",
            "To find the percentage of the total workforce that will be in sales:\n",
            "- Percentage in sales = (Total employees in sales / New total employees) * 100\n",
            "- Percentage in sales = (80 / 130) * 100\n",
            "\n",
            "Let's calculate it:\n",
            "- (80 / 130) = 0.615384615...\n",
            "- 0.615384615... * 100 = 61.5384615...\n",
            "\n",
            "Rounded to two decimal places, the percentage is approximately 61.54%.\n",
            "\n",
            "Therefore, approximately 61.54% of the total workforce will be in sales after the new hires.\n",
            "\n",
            "The final answer is: $\\boxed{61.54}$\n"
          ]
        }
      ],
      "source": [
        "def complex_chain_of_thought():\n",
        "    \"\"\"\n",
        "    Demonstrate complex reasoning with explicit step structure\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Solve this problem by thinking through each step:\n",
        "\n",
        "Problem: A company has 100 employees. 60% work in sales, 25% work in engineering, and the rest work in administration. If the company hires 20 new sales people and 10 new engineers, what percentage of the total workforce will be in sales?\n",
        "\n",
        "Please solve this step by step:\n",
        "Step 1: Calculate current distribution\n",
        "Step 2: Calculate new hires impact\n",
        "Step 3: Calculate new total and sales percentage\n",
        "\n",
        "Solution:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Complex Chain of Thought Result:\")\n",
        "    print(response)\n",
        "\n",
        "complex_chain_of_thought()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CoT Best Practices and Guidelines\n",
        "\n",
        "**Effective CoT Framework Design:**\n",
        "\n",
        "1. **Clear step enumeration**: Number or label each reasoning step\n",
        "2. **Logical flow**: Each step should build naturally on previous steps\n",
        "3. **Explicit instructions**: Tell the model what to think about at each stage\n",
        "4. **Verification points**: Include steps that allow checking intermediate results\n",
        "\n",
        "**When to Use CoT:**\n",
        "- Mathematical calculations with multiple steps\n",
        "- Logical reasoning and analysis\n",
        "- Complex decision-making scenarios\n",
        "- Problems where showing work adds value\n",
        "- Debugging and error analysis tasks\n",
        "\n",
        "**CoT Limitations:**\n",
        "- Longer responses (more tokens/cost)\n",
        "- May over-complicate simple problems\n",
        "- Can hallucinate reasoning steps\n",
        "- Requires careful framework design\n",
        "\n",
        "---\n",
        "\n",
        "## Role Prompting with System Messages\n",
        "\n",
        "Role prompting is a powerful technique that assigns the AI model a specific persona, expertise area, or behavioral pattern. By giving the model a \"role to play,\" you can dramatically influence not just what it says, but how it says it, what knowledge it emphasizes, and what style it adopts.\n",
        "\n",
        "### The Psychology of Role Prompting\n",
        "\n",
        "Role prompting works because:\n",
        "- **Context activation**: The model activates knowledge associated with that role\n",
        "- **Style adaptation**: Communication style matches the assigned persona\n",
        "- **Behavioral consistency**: The role provides behavioral guidelines\n",
        "- **Expertise focus**: Relevant domain knowledge is prioritized\n",
        "\n",
        "### System Messages vs User Messages\n",
        "\n",
        "**System Messages**: \n",
        "- Set persistent behavioral guidelines\n",
        "- Establish the model's persona and constraints\n",
        "- Remain active throughout the conversation\n",
        "- Less likely to be overridden by user instructions\n",
        "\n",
        "**User Messages**: \n",
        "- Provide specific tasks and questions\n",
        "- Can include role information but less persistent\n",
        "- More easily modified or contradicted\n",
        "- Better for one-off instructions\n",
        "\n",
        "### Expert Role Prompting: Technical Authority\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expert Role Response:\n",
            "The bias-variance tradeoff! One of the most fundamental concepts in machine learning. As a senior data scientist, I'm happy to break it down for you.\n",
            "\n",
            "**What is the bias-variance tradeoff?**\n",
            "\n",
            "The bias-variance tradeoff is a property of supervised learning models that describes the inherent tension between two types of errors: bias and variance. In essence, it's a tradeoff between how well a model fits the training data (bias) and how well it generalizes to new, unseen data (variance).\n",
            "\n",
            "**Bias**\n",
            "\n",
            "Bias refers to the difference between the model's expected prediction and the true value. A model with high bias pays little attention to the training data and oversimplifies the relationship between the features and target variable. As a result, it tends to underfit the data, failing to capture important patterns and relationships. Think of bias as a systematic error that occurs when a model is too simple or makes too many assumptions about the data.\n",
            "\n",
            "**Variance**\n",
            "\n",
            "Variance, on the other hand, measures the amount by which the model's predictions vary when it's trained on different datasets. A model with high variance is overly complex and fits the noise in the training data, rather than the underlying patterns. As a result, it tends to overfit the data, performing well on the training set but poorly on new, unseen data. Think of variance as a measure of the model's sensitivity to the specific training data.\n",
            "\n",
            "**The tradeoff**\n",
            "\n",
            "Here's the key insight: as you decrease bias (by making the model more complex), you typically increase variance, and vice versa. This is because a more complex model is better able to fit the training data, but it's also more prone to overfitting and capturing noise. Conversely, a simpler model may not fit the training data as well, but it's less likely to overfit and more likely to generalize to new data.\n",
            "\n",
            "**Consequences for model performance**\n",
            "\n",
            "The bias-variance tradeoff has significant implications for model performance:\n",
            "\n",
            "1. **Underfitting**: A model with high bias and low variance will underfit the data, resulting in poor performance on both the training and test sets.\n",
            "2. **Overfitting**: A model with low bias and high variance will overfit the data, resulting in excellent performance on the training set but poor performance on the test set.\n",
            "3. **Optimal performance**: The ideal model strikes a balance between bias and variance, achieving a sweet spot where it's complex enough to capture the underlying patterns but not so complex that it overfits the data.\n",
            "\n",
            "**Practical implications**\n",
            "\n",
            "To manage the bias-variance tradeoff in practice, you can:\n",
            "\n",
            "1. **Regularization techniques**: Use techniques like L1 and L2 regularization, dropout, or early stopping to reduce overfitting (high variance) by penalizing complex models.\n",
            "2. **Model selection**: Choose models that are robust to overfitting, such as ensemble methods or models with built-in regularization (e.g., random forests, gradient boosting).\n",
            "3. **Hyperparameter tuning**: Optimize hyperparameters to balance bias and variance. For example, increasing the regularization strength can reduce overfitting but may increase bias.\n",
            "4. **Data augmentation**: Increase the size and diversity of the training data to reduce overfitting and improve generalization.\n",
            "5. **Cross-validation**: Use techniques like cross-validation to evaluate model performance on unseen data and detect overfitting.\n",
            "\n",
            "By understanding the bias-variance tradeoff and using these practical strategies, you can develop models that achieve optimal performance and generalize well to new data.\n"
          ]
        }
      ],
      "source": [
        "def expert_role_prompting():\n",
        "    \"\"\"\n",
        "    Demonstrate role prompting by assigning expert persona\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a senior data scientist with 10 years of experience in machine learning and statistics. You explain complex concepts clearly and provide practical insights.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the bias-variance tradeoff in machine learning and how it affects model performance.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Expert Role Response:\")\n",
        "    print(response)\n",
        "\n",
        "expert_role_prompting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Analyzing Expert Role Effectiveness\n",
        "\n",
        "This expert role example demonstrates several key principles:\n",
        "\n",
        "**Credibility Establishment**: \n",
        "- \"Senior data scientist with 10 years of experience\" creates authority\n",
        "- Specific expertise areas (ML and statistics) focus the knowledge domain\n",
        "- Professional background implies practical, not just theoretical, knowledge\n",
        "\n",
        "**Communication Style Guidelines**: \n",
        "- \"Explain complex concepts clearly\" sets an accessibility expectation\n",
        "- \"Provide practical insights\" emphasizes actionable advice\n",
        "- Balances technical depth with clarity\n",
        "\n",
        "**Expected Behavioral Changes**: \n",
        "With this role, you should expect:\n",
        "- More technical terminology used appropriately\n",
        "- References to practical applications and real-world scenarios\n",
        "- Structured explanations with clear examples\n",
        "- Confidence in technical assertions\n",
        "\n",
        "**Topic Complexity**: \n",
        "The bias-variance tradeoff is a sophisticated ML concept that benefits from expert-level explanation. The role ensures the response will be authoritative and comprehensive rather than superficial.\n",
        "\n",
        "### Teacher Role Prompting: Educational Focus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher Role Response:\n",
            "I'm so glad you're starting your programming journey! I'd be happy to help you understand what a function is.\n",
            "\n",
            "**What is a Function?**\n",
            "\n",
            "In programming, a function is a block of code that performs a specific task. Think of it like a recipe: you put in some ingredients (inputs), follow the steps (code), and get a result (output).\n",
            "\n",
            "**Breaking it Down**\n",
            "\n",
            "1. **Inputs**: A function can take in values, called arguments or parameters, which are used to perform the task.\n",
            "2. **Code**: The function contains a set of instructions that are executed when the function is called.\n",
            "3. **Output**: A function can return a result, which can be used by the rest of the program.\n",
            "\n",
            "**Example**\n",
            "\n",
            "Let's consider a simple example: a function that greets someone by name.\n",
            "\n",
            "* Input: a person's name (e.g., \"John\")\n",
            "* Code: the function uses the input name to construct a greeting message\n",
            "* Output: the greeting message (e.g., \"Hello, John!\")\n",
            "\n",
            "In code, this might look like:\n",
            "```python\n",
            "def greet(name):\n",
            "    message = \"Hello, \" + name + \"!\"\n",
            "    return message\n",
            "\n",
            "print(greet(\"John\"))  # Output: \"Hello, John!\"\n",
            "```\n",
            "**Why Use Functions?**\n",
            "\n",
            "Functions are helpful because they:\n",
            "\n",
            "* Make code reusable: you can call the same function multiple times with different inputs.\n",
            "* Organize code: functions help break down a large program into smaller, manageable pieces.\n",
            "* Improve readability: functions give a clear name to a block of code, making it easier to understand what the code does.\n",
            "\n",
            "**Do you Understand?**\n",
            "\n",
            "Can you think of a simple task that you could turn into a function? For example, a function that adds two numbers together or one that converts a temperature from Celsius to Fahrenheit.\n",
            "\n",
            "Do you have any questions about functions so far?\n"
          ]
        }
      ],
      "source": [
        "def teacher_role_prompting():\n",
        "    \"\"\"\n",
        "    Demonstrate role prompting as an educational instructor\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a patient and encouraging programming teacher. You break down complex concepts into simple steps, provide examples, and always check for understanding.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I'm a beginner. Can you explain what a function is in programming?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Teacher Role Response:\")\n",
        "    print(response)\n",
        "\n",
        "teacher_role_prompting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Educational Role Design Principles\n",
        "\n",
        "The teacher role demonstrates different communication priorities:\n",
        "\n",
        "**Pedagogical Approach**: \n",
        "- \"Patient and encouraging\" sets emotional tone\n",
        "- \"Break down complex concepts into simple steps\" defines methodology\n",
        "- \"Always check for understanding\" ensures comprehension\n",
        "\n",
        "**Beginner-Friendly Characteristics**: \n",
        "- Avoids jargon without explanation\n",
        "- Uses analogies and real-world examples\n",
        "- Provides step-by-step progression\n",
        "- Encourages questions and interaction\n",
        "\n",
        "**Contrast with Expert Role**: \n",
        "- **Expert**: Assumes knowledge, provides depth, uses technical language\n",
        "- **Teacher**: Assumes no prior knowledge, builds understanding gradually\n",
        "- **Expert**: Efficient, comprehensive coverage\n",
        "- **Teacher**: Patient, thorough explanation of basics\n",
        "\n",
        "**Question Choice**: \n",
        "\"What is a function in programming?\" is fundamental but can be explained at many levels. The teacher role ensures an accessible, beginner-appropriate explanation.\n",
        "\n",
        "### Business Analyst Role: Structured Professional Output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyst Role Response:\n",
            "**4-Day Work Week Analysis: Tech Company Implementation**\n",
            "\n",
            "### Executive Summary\n",
            "\n",
            "Implementing a 4-day work week in a tech company can have both positive and negative impacts on the organization. This analysis highlights the key pros and cons, providing a comprehensive overview to inform business decisions.\n",
            "\n",
            "### Pros of a 4-Day Work Week\n",
            "\n",
            "* **Increased Employee Satisfaction and Retention**\n",
            "\t+ Improved work-life balance\n",
            "\t+ Enhanced productivity and focus during working hours\n",
            "\t+ Potential reduction in turnover rates\n",
            "* **Cost Savings**\n",
            "\t+ Reduced overhead costs (e.g., utilities, facilities maintenance)\n",
            "\t+ Lower commuting costs for employees\n",
            "* **Talent Attraction and Competitive Advantage**\n",
            "\t+ Unique benefit to attract top talent in a competitive job market\n",
            "\t+ Differentiation from traditional 5-day work week companies\n",
            "* **Environmental Benefits**\n",
            "\t+ Reduced carbon footprint due to decreased commuting\n",
            "\n",
            "### Cons of a 4-Day Work Week\n",
            "\n",
            "* **Potential Impact on Customer Service and Support**\n",
            "\t+ Reduced availability for customer support and service\n",
            "\t+ Potential negative impact on customer satisfaction\n",
            "* **Operational Challenges**\n",
            "\t+ Compressed workweek may require adjustments to workflows and processes\n",
            "\t+ Potential impact on project timelines and delivery\n",
            "* **Inequitable Distribution of Workload**\n",
            "\t+ Potential for uneven workload distribution among employees\n",
            "\t+ Increased workload for some employees to maintain productivity\n",
            "* **Monitoring and Measuring Productivity**\n",
            "\t+ Challenges in tracking and measuring productivity in a compressed workweek\n",
            "\n",
            "### Recommendations\n",
            "\n",
            "1. **Conduct a Pilot Program**: Implement a trial 4-day work week for a select group of employees or teams to assess the impact on productivity, customer satisfaction, and operational efficiency.\n",
            "2. **Adjust Workflows and Processes**: Review and adapt workflows, processes, and project management methodologies to ensure a smooth transition to a 4-day work week.\n",
            "3. **Establish Clear Productivity Metrics**: Develop and track key performance indicators (KPIs) to measure productivity, efficiency, and customer satisfaction during the compressed workweek.\n",
            "4. **Communicate with Stakeholders**: Inform customers, employees, and stakeholders about the 4-day work week implementation, highlighting the benefits and addressing potential concerns.\n",
            "5. **Review and Revise**: Continuously monitor the impact of the 4-day work week and make adjustments as needed to ensure the benefits outweigh the drawbacks.\n",
            "\n",
            "### Implementation Roadmap\n",
            "\n",
            "1. **Pre-Implementation (Weeks 1-4)**\n",
            "\t* Conduct a pilot program\n",
            "\t* Review and adjust workflows and processes\n",
            "\t* Establish clear productivity metrics\n",
            "2. **Implementation (Weeks 5-8)**\n",
            "\t* Roll out the 4-day work week to the entire organization\n",
            "\t* Communicate with stakeholders\n",
            "\t* Monitor and track productivity metrics\n",
            "3. **Post-Implementation (After Week 8)**\n",
            "\t* Review the impact of the 4-day work week\n",
            "\t* Make adjustments as needed\n",
            "\t* Continuously monitor and evaluate the effectiveness of the compressed workweek\n",
            "\n",
            "By following this structured approach, tech companies can effectively analyze the pros and cons of implementing a 4-day work week and make informed decisions to drive business success.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'**4-Day Work Week Analysis: Tech Company Implementation**\\n\\n### Executive Summary\\n\\nImplementing a 4-day work week in a tech company can have both positive and negative impacts on the organization. This analysis highlights the key pros and cons, providing a comprehensive overview to inform business decisions.\\n\\n### Pros of a 4-Day Work Week\\n\\n* **Increased Employee Satisfaction and Retention**\\n\\t+ Improved work-life balance\\n\\t+ Enhanced productivity and focus during working hours\\n\\t+ Potential reduction in turnover rates\\n* **Cost Savings**\\n\\t+ Reduced overhead costs (e.g., utilities, facilities maintenance)\\n\\t+ Lower commuting costs for employees\\n* **Talent Attraction and Competitive Advantage**\\n\\t+ Unique benefit to attract top talent in a competitive job market\\n\\t+ Differentiation from traditional 5-day work week companies\\n* **Environmental Benefits**\\n\\t+ Reduced carbon footprint due to decreased commuting\\n\\n### Cons of a 4-Day Work Week\\n\\n* **Potential Impact on Customer Service and Support**\\n\\t+ Reduced availability for customer support and service\\n\\t+ Potential negative impact on customer satisfaction\\n* **Operational Challenges**\\n\\t+ Compressed workweek may require adjustments to workflows and processes\\n\\t+ Potential impact on project timelines and delivery\\n* **Inequitable Distribution of Workload**\\n\\t+ Potential for uneven workload distribution among employees\\n\\t+ Increased workload for some employees to maintain productivity\\n* **Monitoring and Measuring Productivity**\\n\\t+ Challenges in tracking and measuring productivity in a compressed workweek\\n\\n### Recommendations\\n\\n1. **Conduct a Pilot Program**: Implement a trial 4-day work week for a select group of employees or teams to assess the impact on productivity, customer satisfaction, and operational efficiency.\\n2. **Adjust Workflows and Processes**: Review and adapt workflows, processes, and project management methodologies to ensure a smooth transition to a 4-day work week.\\n3. **Establish Clear Productivity Metrics**: Develop and track key performance indicators (KPIs) to measure productivity, efficiency, and customer satisfaction during the compressed workweek.\\n4. **Communicate with Stakeholders**: Inform customers, employees, and stakeholders about the 4-day work week implementation, highlighting the benefits and addressing potential concerns.\\n5. **Review and Revise**: Continuously monitor the impact of the 4-day work week and make adjustments as needed to ensure the benefits outweigh the drawbacks.\\n\\n### Implementation Roadmap\\n\\n1. **Pre-Implementation (Weeks 1-4)**\\n\\t* Conduct a pilot program\\n\\t* Review and adjust workflows and processes\\n\\t* Establish clear productivity metrics\\n2. **Implementation (Weeks 5-8)**\\n\\t* Roll out the 4-day work week to the entire organization\\n\\t* Communicate with stakeholders\\n\\t* Monitor and track productivity metrics\\n3. **Post-Implementation (After Week 8)**\\n\\t* Review the impact of the 4-day work week\\n\\t* Make adjustments as needed\\n\\t* Continuously monitor and evaluate the effectiveness of the compressed workweek\\n\\nBy following this structured approach, tech companies can effectively analyze the pros and cons of implementing a 4-day work week and make informed decisions to drive business success.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def analyst_role_prompting():\n",
        "    \"\"\"\n",
        "    Demonstrate role prompting with specific constraints and style\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a business analyst who always provides structured responses with clear headings, bullet points, and actionable recommendations. You focus on practical business implications.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Analyze the pros and cons of implementing a 4-day work week in a tech company.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Analyst Role Response:\")\n",
        "    print(response)\n",
        "    return response\n",
        "\n",
        "analyst_role_prompting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Professional Role Engineering\n",
        "\n",
        "The business analyst role showcases advanced role prompting techniques:\n",
        "\n",
        "**Dual Role Specification**: \n",
        "- **Professional identity**: \"Business analyst\" sets domain expertise\n",
        "- **Communication style**: Structured responses with specific formatting\n",
        "\n",
        "**Format Requirements**: \n",
        "- \"Clear headings\" - organizes information hierarchically\n",
        "- \"Bullet points\" - makes content scannable and digestible\n",
        "- \"Actionable recommendations\" - ensures practical utility\n",
        "\n",
        "**Business Focus**: \n",
        "- \"Practical business implications\" prioritizes real-world impact\n",
        "- Emphasizes decision-making utility over theoretical analysis\n",
        "- Results-oriented rather than academic\n",
        "\n",
        "**Expected Output Structure**: \n",
        "With this role, expect responses like:\n",
        "- **Executive Summary**\n",
        "- **Pros and Cons** (bulleted lists)\n",
        "- **Key Considerations**\n",
        "- **Recommendations** (specific, actionable)\n",
        "- **Implementation Notes**\n",
        "\n",
        "### Role Prompting Best Practices\n",
        "\n",
        "**Effective Role Design**: \n",
        "\n",
        "1. **Specific expertise**: Define clear knowledge domains\n",
        "2. **Communication style**: Specify how the role should \"speak\"\n",
        "3. **Behavioral guidelines**: Include personality and approach characteristics\n",
        "4. **Output format**: Define structure and presentation requirements\n",
        "5. **Constraints**: Specify what the role should avoid or emphasize\n",
        "\n",
        "**Role Selection Strategy**: \n",
        "- **Match expertise to task complexity**: Technical roles for technical tasks\n",
        "- **Consider your audience**: Teacher role for beginners, expert role for specialists\n",
        "- **Define output needs**: Analyst role for structured business documents\n",
        "- **Think about tone**: Professional, casual, academic, creative\n",
        "\n",
        "**Common Role Types**: \n",
        "- **Subject matter experts**: Doctor, lawyer, engineer, scientist\n",
        "- **Communication specialists**: Teacher, journalist, technical writer\n",
        "- **Business professionals**: Analyst, consultant, manager, strategist\n",
        "- **Creative roles**: Writer, designer, brainstorming partner\n",
        "\n",
        "---\n",
        "\n",
        "## Data Cleaning with Prompt Engineering\n",
        "\n",
        "Data cleaning is one of the most practical and immediately valuable applications of prompt engineering in business contexts. Real-world data is messy, inconsistent, and often requires significant preprocessing before it can be used for analysis or machine learning. This section demonstrates how different prompt engineering techniques can be applied to transform chaotic data into clean, standardized formats.\n",
        "\n",
        "### Why Data Cleaning Matters\n",
        "\n",
        "Data scientists often spend 60-80% of their time cleaning and preparing data. Common data quality issues include:\n",
        "- **Inconsistent formatting**: Names in different cases, phone numbers in various formats\n",
        "- **Missing values**: Empty fields, \"unknown\" entries, inconsistent null representations\n",
        "- **Type inconsistencies**: Ages as words instead of numbers, dates in different formats\n",
        "- **Standardization needs**: Job titles that mean the same thing but are written differently\n",
        "\n",
        "### The Power of LLMs for Data Cleaning\n",
        "\n",
        "Large Language Models excel at data cleaning because they:\n",
        "- **Understand context**: Can infer what \"thirty-two\" means in an age field\n",
        "- **Handle ambiguity**: Can make reasonable decisions about edge cases\n",
        "- **Recognize patterns**: Can standardize variations of the same information\n",
        "- **Apply business logic**: Can make domain-specific cleaning decisions\n",
        "\n",
        "### Working with Pandas DataFrames\n",
        "\n",
        "Throughout this section, we'll use pandas DataFrames because they're the standard tool for data manipulation in Python. This approach demonstrates how prompt engineering integrates with existing data science workflows.\n",
        "\n",
        "### Creating Our Messy Dataset\n",
        "\n",
        "Let's start by creating a dataset that represents common real-world data quality issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messy Dataset as DataFrame:\n",
            "                                                                raw_record  record_id\n",
            "         john doe, 25, software engineer, john.doe@email.com, 555-123-4567          1\n",
            "           JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543          2\n",
            "   Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876          3\n",
            "mary williams, 28, Data Scientist, mary.williams@company.com, 555-456-7890          4\n",
            "          MIKE BROWN, 45, HR Manager, mike@hr.company.com, +1-555-234-5678          5\n",
            "         sarah davis, unknown, developer, sarah.davis@tech.com, 5551234567          6\n",
            "     Tom Wilson, 35, Product Manager, tom.wilson@company.com, 555-876-5432          7\n",
            "             lisa garcia, 29, UX Designer, lisa@design.com, (555) 345-6789          8\n",
            "\n",
            "Dataset shape: (8, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def create_messy_dataframe():\n",
        "    \"\"\"\n",
        "    Create a sample messy dataset as a pandas DataFrame\n",
        "    \"\"\"\n",
        "    messy_data = {\n",
        "        'raw_record': [\n",
        "            \"john doe, 25, software engineer, john.doe@email.com, 555-123-4567\",\n",
        "            \"JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543\",\n",
        "            \"Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876\",\n",
        "            \"mary williams, 28, Data Scientist, mary.williams@company.com, 555-456-7890\",\n",
        "            \"MIKE BROWN, 45, HR Manager, mike@hr.company.com, +1-555-234-5678\",\n",
        "            \"sarah davis, unknown, developer, sarah.davis@tech.com, 5551234567\",\n",
        "            \"Tom Wilson, 35, Product Manager, tom.wilson@company.com, 555-876-5432\",\n",
        "            \"lisa garcia, 29, UX Designer, lisa@design.com, (555) 345-6789\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(messy_data)\n",
        "    df['record_id'] = range(1, len(df) + 1)\n",
        "    \n",
        "    print(\"Messy Dataset as DataFrame:\")\n",
        "    print(df.to_string(index=False))\n",
        "    print(f\"\\nDataset shape: {df.shape}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "messy_df = create_messy_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Understanding Our Data Quality Issues\n",
        "\n",
        "Our sample dataset contains typical real-world problems:\n",
        "\n",
        "**Naming Inconsistencies**: \n",
        "- \"john doe\" (all lowercase)\n",
        "- \"JANE SMITH\" (all uppercase)\n",
        "- \"Bob Johnson\" (proper case)\n",
        "- \"mary williams\" (lowercase)\n",
        "\n",
        "**Age Format Variations**: \n",
        "- \"25\" (numeric)\n",
        "- \"\" (missing/empty)\n",
        "- \"thirty-two\" (written out)\n",
        "- \"unknown\" (explicit unknown value)\n",
        "\n",
        "**Job Title Inconsistencies**: \n",
        "- \"software engineer\" vs \"developer\" (same role, different terms)\n",
        "- \"Sales Rep\" vs \"marketing manager\" (different cases)\n",
        "- \"Data Scientist\" (proper case)\n",
        "\n",
        "**Phone Number Chaos**: \n",
        "- \"555-123-4567\" (standard format)\n",
        "- \"(555) 987-6543\" (parentheses format)\n",
        "- \"555.321.9876\" (dot separators)\n",
        "- \"+1-555-234-5678\" (international format)\n",
        "- \"5551234567\" (no separators)\n",
        "\n",
        "**Email Variations**: \n",
        "- Different domains and formats\n",
        "- Mixed case in usernames\n",
        "\n",
        "This diversity mirrors what you'll encounter in real business data, making our examples immediately applicable to practical scenarios.\n",
        "\n",
        "### Helper Functions for DataFrame Integration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined for DataFrame operations.\n"
          ]
        }
      ],
      "source": [
        "def parse_cleaned_json(response_text):\n",
        "    \"\"\"\n",
        "    Extract JSON from LLM response and handle parsing errors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to find JSON in the response\n",
        "        start_idx = response_text.find('{')\n",
        "        end_idx = response_text.rfind('}') + 1\n",
        "        \n",
        "        if start_idx != -1 and end_idx != 0:\n",
        "            json_str = response_text[start_idx:end_idx]\n",
        "            return json.loads(json_str)\n",
        "        else:\n",
        "            return None\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "def apply_cleaning_to_dataframe(df, cleaning_function, column_name='raw_record'):\n",
        "    \"\"\"\n",
        "    Apply a cleaning function to each row in the DataFrame\n",
        "    \"\"\"\n",
        "    cleaned_results = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"Processing record {idx + 1}...\")\n",
        "        response = cleaning_function(row[column_name])\n",
        "        parsed_json = parse_cleaned_json(response)\n",
        "        \n",
        "        if parsed_json:\n",
        "            cleaned_results.append(parsed_json)\n",
        "        else:\n",
        "            # Fallback for unparseable responses\n",
        "            cleaned_results.append({\n",
        "                'name': None,\n",
        "                'age': None,\n",
        "                'job_title': None,\n",
        "                'email': None,\n",
        "                'phone': None,\n",
        "                'error': 'Failed to parse response'\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(cleaned_results)\n",
        "\n",
        "print(\"Helper functions defined for DataFrame operations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Understanding Our Data Processing Pipeline\n",
        "\n",
        "These helper functions create a robust data processing framework:\n",
        "\n",
        "**JSON Parsing with Error Handling**: \n",
        "The `parse_cleaned_json` function handles the common challenge of extracting structured data from LLM responses:\n",
        "- **Flexible parsing**: Finds JSON anywhere in the response text\n",
        "- **Error resilience**: Returns None instead of crashing on invalid JSON\n",
        "- **Real-world adaptation**: LLMs sometimes include explanatory text around the JSON\n",
        "\n",
        "**DataFrame Integration**: \n",
        "The `apply_cleaning_to_dataframe` function bridges prompt engineering with pandas workflows:\n",
        "- **Row-by-row processing**: Applies cleaning functions to each record\n",
        "- **Progress tracking**: Shows processing status for long datasets\n",
        "- **Fallback handling**: Creates error records when parsing fails\n",
        "- **Consistent output**: Always returns a DataFrame, even with errors\n",
        "\n",
        "**Production Considerations**: \n",
        "- **Error tracking**: Captures failures without stopping the entire process\n",
        "- **Data integrity**: Preserves original data while adding cleaned versions\n",
        "- **Scalability**: Framework can handle datasets of various sizes\n",
        "\n",
        "### Zero-Shot Data Cleaning: Direct Instruction Approach\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing record 1...\n",
            "Processing record 2...\n",
            "Processing record 3...\n",
            "\n",
            "Original records:\n",
            " record_id                                                              raw_record\n",
            "         1       john doe, 25, software engineer, john.doe@email.com, 555-123-4567\n",
            "         2         JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543\n",
            "         3 Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876\n",
            "\n",
            "Cleaned records:\n",
            "       name  age         job_title                 email        phone\n",
            "   John Doe 25.0 Software Engineer    john.doe@email.com 555-123-4567\n",
            " Jane Smith  NaN Marketing Manager       jane@company.co 555-987-6543\n",
            "Bob Johnson  NaN         Sales Rep bob.johnson@gmail.com 555-321-9876\n",
            "\n",
            "Combined DataFrame:\n",
            "                                                             raw_record  record_id        name  age         job_title                 email        phone\n",
            "      john doe, 25, software engineer, john.doe@email.com, 555-123-4567          1    John Doe 25.0 Software Engineer    john.doe@email.com 555-123-4567\n",
            "        JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543          2  Jane Smith  NaN Marketing Manager       jane@company.co 555-987-6543\n",
            "Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876          3 Bob Johnson  NaN         Sales Rep bob.johnson@gmail.com 555-321-9876\n"
          ]
        }
      ],
      "source": [
        "def zero_shot_data_cleaning_single(record):\n",
        "    \"\"\"\n",
        "    Use zero-shot prompting to clean a single data record\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Clean and standardize this data record. Return ONLY valid JSON with fields: name, age, job_title, email, phone.\n",
        "\n",
        "Raw data: \"{record}\"\n",
        "\n",
        "Requirements:\n",
        "- Name: Title case (First Last)\n",
        "- Age: Number only (if unknown, use null)\n",
        "- Job title: Title case\n",
        "- Email: Lowercase\n",
        "- Phone: Format as XXX-XXX-XXXX\n",
        "\n",
        "Return only the JSON object:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    return call_llama(messages)\n",
        "\n",
        "# Apply zero-shot cleaning to first 3 records\n",
        "sample_df = messy_df.head(3).copy()\n",
        "cleaned_sample = apply_cleaning_to_dataframe(sample_df, zero_shot_data_cleaning_single)\n",
        "\n",
        "print(\"\\nOriginal records:\")\n",
        "print(sample_df[['record_id', 'raw_record']].to_string(index=False))\n",
        "print(\"\\nCleaned records:\")\n",
        "print(cleaned_sample.to_string(index=False))\n",
        "\n",
        "# Combine original and cleaned data\n",
        "result_df = pd.concat([sample_df.reset_index(drop=True), cleaned_sample], axis=1)\n",
        "print(\"\\nCombined DataFrame:\")\n",
        "print(result_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Zero-Shot Data Cleaning Analysis\n",
        "\n",
        "This zero-shot approach demonstrates several important principles:\n",
        "\n",
        "**Clear Requirements Specification**: \n",
        "- **Output format**: \"Return ONLY valid JSON\" eliminates ambiguity\n",
        "- **Field specification**: Lists exact field names expected\n",
        "- **Format standards**: Defines specific formatting rules for each field type\n",
        "\n",
        "**Standardization Rules**: \n",
        "- **Name**: Title case ensures consistent capitalization\n",
        "- **Age**: Number or null handles various input formats\n",
        "- **Job title**: Title case standardizes professional titles\n",
        "- **Email**: Lowercase follows internet standards\n",
        "- **Phone**: XXX-XXX-XXXX creates uniform format\n",
        "\n",
        "**Business Logic Integration**: \n",
        "The prompt embeds common business rules:\n",
        "- Handle \"unknown\" as null values\n",
        "- Standardize phone formats for database storage\n",
        "- Apply consistent capitalization for professional presentation\n",
        "\n",
        "**Why This Works**: \n",
        "Zero-shot cleaning leverages the model's training on similar data transformation tasks. The clear specification helps the model understand the exact transformation needed.\n",
        "\n",
        "**DataFrame Integration Benefits**: \n",
        "- **Side-by-side comparison**: Original and cleaned data are easily comparable\n",
        "- **Quality assessment**: You can immediately see cleaning effectiveness\n",
        "- **Further processing**: Cleaned DataFrames integrate with existing pandas workflows\n",
        "\n",
        "### Few-Shot Data Cleaning: Learning from Examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot Data Cleaning Results:\n",
            "\n",
            "Target records:\n",
            " record_id                                                                 raw_record\n",
            "         3    Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876\n",
            "         4 mary williams, 28, Data Scientist, mary.williams@company.com, 555-456-7890\n",
            "         5           MIKE BROWN, 45, HR Manager, mike@hr.company.com, +1-555-234-5678\n",
            "\n",
            "Cleaned results:\n",
            "         name  age            job_title                     email           phone\n",
            "  Bob Johnson   32 Sales Representative     bob.johnson@gmail.com    555-321-9876\n",
            "Mary Williams   28       Data Scientist mary.williams@company.com    555-456-7890\n",
            "   Mike Brown   45           HR Manager       mike@hr.company.com +1-555-234-5678\n"
          ]
        }
      ],
      "source": [
        "def few_shot_data_cleaning_batch(df, num_examples=2):\n",
        "    \"\"\"\n",
        "    Use few-shot prompting to clean multiple records with examples\n",
        "    \"\"\"\n",
        "    # Use first few records as examples\n",
        "    examples = df.head(num_examples)\n",
        "    target_records = df.iloc[num_examples:num_examples+3]  # Next 3 records\n",
        "    \n",
        "    # Create examples text\n",
        "    examples_text = \"\"\n",
        "    for i, (_, row) in enumerate(examples.iterrows(), 1):\n",
        "        examples_text += f\"\"\"Example {i}:\n",
        "Raw: \"{row['raw_record']}\"\n",
        "Cleaned: {{\"name\": \"John Doe\", \"age\": 25, \"job_title\": \"Software Engineer\", \"email\": \"john.doe@email.com\", \"phone\": \"555-123-4567\"}}\n",
        "\n",
        "\"\"\"\n",
        "    \n",
        "    cleaned_results = []\n",
        "    \n",
        "    for _, row in target_records.iterrows():\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Clean and standardize data records following these examples:\n",
        "\n",
        "{examples_text}\n",
        "Now clean this record (return ONLY the JSON):\n",
        "Raw: \"{row['raw_record']}\"\n",
        "Cleaned:\"\"\"\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        response = call_llama(messages)\n",
        "        parsed_json = parse_cleaned_json(response)\n",
        "        \n",
        "        if parsed_json:\n",
        "            cleaned_results.append(parsed_json)\n",
        "        else:\n",
        "            cleaned_results.append({'error': 'Failed to parse'})\n",
        "    \n",
        "    cleaned_df = pd.DataFrame(cleaned_results)\n",
        "    \n",
        "    print(\"Few-shot Data Cleaning Results:\")\n",
        "    print(\"\\nTarget records:\")\n",
        "    print(target_records[['record_id', 'raw_record']].to_string(index=False))\n",
        "    print(\"\\nCleaned results:\")\n",
        "    print(cleaned_df.to_string(index=False))\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "few_shot_results = few_shot_data_cleaning_batch(messy_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Few-Shot Data Cleaning Advantages\n",
        "\n",
        "The few-shot approach offers significant improvements over zero-shot:\n",
        "\n",
        "**Pattern Learning**: \n",
        "- **Concrete examples**: Shows exact input-output transformations\n",
        "- **Format consistency**: Establishes precise JSON structure\n",
        "- **Edge case handling**: Examples demonstrate how to handle missing data\n",
        "\n",
        "**Example Strategy**: \n",
        "- **Example 1**: Clean, standard input → standard output\n",
        "- **Example 2**: Missing age field → null value handling\n",
        "- **Coverage**: Examples represent different types of data quality issues\n",
        "\n",
        "**Consistency Benefits**: \n",
        "- **Reduced variation**: Examples constrain output format\n",
        "- **Standardized field names**: Prevents field naming inconsistencies\n",
        "- **Predictable data types**: Examples establish type conventions\n",
        "\n",
        "**Business Value**: \n",
        "Few-shot cleaning is particularly valuable when:\n",
        "- You need consistent output across large datasets\n",
        "- Your data has domain-specific cleaning requirements\n",
        "- You want to establish organizational data standards\n",
        "\n",
        "**Scalability Considerations**: \n",
        "While this example processes records individually, the pattern established by examples ensures consistency across the entire dataset.\n",
        "\n",
        "### Chain of Thought Data Cleaning: Reasoning Through Decisions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain of Thought Data Cleaning for Record 6:\n",
            "Raw: sarah davis, unknown, developer, sarah.davis@tech.com, 5551234567\n",
            "\n",
            "LLM Response with Reasoning:\n",
            "Let's go through the steps to clean the data record.\n",
            "\n",
            "### Step 1: Parse the fields\n",
            "\n",
            "The raw data is: \"sarah davis, unknown, developer, sarah.davis@tech.com, 5551234567\"\n",
            "\n",
            "We can identify the fields as follows:\n",
            "\n",
            "- Name: \"sarah davis\"\n",
            "- Age: \"unknown\"\n",
            "- Job: \"developer\"\n",
            "- Email: \"sarah.davis@tech.com\"\n",
            "- Phone: \"5551234567\"\n",
            "\n",
            "### Step 2: Standardize name (proper case)\n",
            "\n",
            "The name is \"sarah davis\". To standardize it to proper case, we capitalize the first letter of each word.\n",
            "\n",
            "- Standardized Name: \"Sarah Davis\"\n",
            "\n",
            "### Step 3: Handle age (convert to number or null if unknown)\n",
            "\n",
            "The age is given as \"unknown\". Since it's not a valid number, we will convert it to `null`.\n",
            "\n",
            "- Age: `null`\n",
            "\n",
            "### Step 4: Standardize job title (proper case)\n",
            "\n",
            "The job title is \"developer\". To standardize it to proper case, we capitalize the first letter.\n",
            "\n",
            "- Standardized Job: \"Developer\"\n",
            "\n",
            "### Step 5: Standardize email (lowercase)\n",
            "\n",
            "The email is \"sarah.davis@tech.com\". It's already in lowercase, so no change is needed.\n",
            "\n",
            "- Standardized Email: \"sarah.davis@tech.com\"\n",
            "\n",
            "### Step 6: Standardize phone (XXX-XXX-XXXX format)\n",
            "\n",
            "The phone number is \"5551234567\". To standardize it to the XXX-XXX-XXXX format, we insert hyphens.\n",
            "\n",
            "- Standardized Phone: \"555-123-4567\"\n",
            "\n",
            "### Step 7: Output final JSON\n",
            "\n",
            "Now, we compile the cleaned data into a JSON object.\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Record ID\": 6,\n",
            "  \"Name\": \"Sarah Davis\",\n",
            "  \"Age\": null,\n",
            "  \"Job\": \"Developer\",\n",
            "  \"Email\": \"sarah.davis@tech.com\",\n",
            "  \"Phone\": \"555-123-4567\"\n",
            "}\n",
            "```\n",
            "\n",
            "The final JSON is the cleaned and standardized version of the original raw data record.\n",
            "\n",
            "Extracted cleaned data:\n",
            " Record ID        Name  Age       Job                Email        Phone\n",
            "         6 Sarah Davis None Developer sarah.davis@tech.com 555-123-4567\n"
          ]
        }
      ],
      "source": [
        "def cot_data_cleaning_with_reasoning(df, record_index=5):\n",
        "    \"\"\"\n",
        "    Use chain of thought reasoning for complex data cleaning decisions\n",
        "    \"\"\"\n",
        "    target_record = df.iloc[record_index]\n",
        "    \n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Clean this data record step by step, explaining your reasoning:\n",
        "\n",
        "Record ID: {target_record['record_id']}\n",
        "Raw data: \"{target_record['raw_record']}\"\n",
        "\n",
        "Please follow this process:\n",
        "1. Parse the fields (identify name, age, job, email, phone)\n",
        "2. Standardize name (proper case)\n",
        "3. Handle age (convert to number or null if unknown)\n",
        "4. Standardize job title (proper case)\n",
        "5. Standardize email (lowercase)\n",
        "6. Standardize phone (XXX-XXX-XXXX format)\n",
        "7. Output final JSON\n",
        "\n",
        "Show your step-by-step reasoning, then provide the final JSON:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(f\"Chain of Thought Data Cleaning for Record {target_record['record_id']}:\")\n",
        "    print(f\"Raw: {target_record['raw_record']}\")\n",
        "    print(\"\\nLLM Response with Reasoning:\")\n",
        "    print(response)\n",
        "    \n",
        "    # Extract the final JSON\n",
        "    parsed_json = parse_cleaned_json(response)\n",
        "    if parsed_json:\n",
        "        cleaned_df = pd.DataFrame([parsed_json])\n",
        "        print(\"\\nExtracted cleaned data:\")\n",
        "        print(cleaned_df.to_string(index=False))\n",
        "        return cleaned_df\n",
        "    else:\n",
        "        print(\"\\nFailed to extract JSON from response\")\n",
        "        return None\n",
        "\n",
        "cot_result = cot_data_cleaning_with_reasoning(messy_df, record_index=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Chain of Thought Data Cleaning Benefits\n",
        "\n",
        "CoT data cleaning provides unique advantages for complex data quality scenarios:\n",
        "\n",
        "**Decision Transparency**: \n",
        "- **Step-by-step reasoning**: Shows how each cleaning decision was made\n",
        "- **Rule application**: Demonstrates how business rules are applied\n",
        "- **Error detection**: Makes it easier to spot and fix cleaning logic errors\n",
        "\n",
        "**Complex Decision Making**: \n",
        "This approach excels when cleaning decisions require:\n",
        "- **Contextual analysis**: Understanding what \"unknown\" means in an age field\n",
        "- **Multi-step transformations**: Phone number parsing and reformatting\n",
        "- **Ambiguity resolution**: Deciding how to handle edge cases\n",
        "\n",
        "**Quality Assurance**: \n",
        "- **Auditable process**: Each step can be verified independently\n",
        "- **Learning opportunity**: Understanding reasoning helps improve future prompts\n",
        "- **Debugging support**: When cleaning fails, you can see where the logic broke down\n",
        "\n",
        "**Business Applications**: \n",
        "CoT cleaning is particularly valuable for:\n",
        "- **Regulated industries**: Where data transformation must be documented\n",
        "- **High-stakes data**: Where cleaning errors have significant consequences\n",
        "- **Training scenarios**: Where understanding the process is as important as the result\n",
        "\n",
        "**Record Selection**: \n",
        "We chose record 5 (sarah davis, unknown, developer...) because it contains multiple edge cases that benefit from step-by-step reasoning.\n",
        "\n",
        "### Batch Processing: Entire DataFrame Consistency\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Cleaning Response:\n",
            "```json\n",
            "[\n",
            "  {\"name\": \"John Doe\", \"age\": 25, \"jobTitle\": \"Software Engineer\", \"email\": \"john.doe@email.com\", \"phone\": \"555-123-4567\"},\n",
            "  {\"name\": \"Jane Smith\", \"age\": null, \"jobTitle\": \"Marketing Manager\", \"email\": \"jane@company.co\", \"phone\": \"555-987-6543\"},\n",
            "  {\"name\": \"Bob Johnson\", \"age\": null, \"jobTitle\": \"Sales Representative\", \"email\": \"bob.johnson@gmail.com\", \"phone\": \"555-321-9876\"},\n",
            "  {\"name\": \"Mary Williams\", \"age\": 28, \"jobTitle\": \"Data Scientist\", \"email\": \"mary.williams@company.com\",...\n",
            "\n",
            "Successfully parsed 8 records\n",
            "\n",
            "Batch Cleaned DataFrame:\n",
            "         name  age             jobTitle                     email        phone\n",
            "     John Doe 25.0    Software Engineer        john.doe@email.com 555-123-4567\n",
            "   Jane Smith  NaN    Marketing Manager           jane@company.co 555-987-6543\n",
            "  Bob Johnson  NaN Sales Representative     bob.johnson@gmail.com 555-321-9876\n",
            "Mary Williams 28.0       Data Scientist mary.williams@company.com 555-456-7890\n",
            "   Mike Brown 45.0           HR Manager       mike@hr.company.com 555-234-5678\n",
            "  Sarah Davis  NaN    Software Engineer      sarah.davis@tech.com 555-123-4567\n",
            "   Tom Wilson 35.0      Product Manager    tom.wilson@company.com 555-876-5432\n",
            "  Lisa Garcia 29.0          UX Designer           lisa@design.com 555-345-6789\n",
            "\n",
            "Side-by-side comparison:\n",
            " record_id                                                                 raw_record          name  age             jobTitle                     email        phone\n",
            "         1          john doe, 25, software engineer, john.doe@email.com, 555-123-4567      John Doe 25.0    Software Engineer        john.doe@email.com 555-123-4567\n",
            "         2            JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543    Jane Smith  NaN    Marketing Manager           jane@company.co 555-987-6543\n",
            "         3    Bob Johnson, thirty-two, Sales Rep, bob.johnson@gmail.com, 555.321.9876   Bob Johnson  NaN Sales Representative     bob.johnson@gmail.com 555-321-9876\n",
            "         4 mary williams, 28, Data Scientist, mary.williams@company.com, 555-456-7890 Mary Williams 28.0       Data Scientist mary.williams@company.com 555-456-7890\n",
            "         5           MIKE BROWN, 45, HR Manager, mike@hr.company.com, +1-555-234-5678    Mike Brown 45.0           HR Manager       mike@hr.company.com 555-234-5678\n",
            "         6          sarah davis, unknown, developer, sarah.davis@tech.com, 5551234567   Sarah Davis  NaN    Software Engineer      sarah.davis@tech.com 555-123-4567\n",
            "         7      Tom Wilson, 35, Product Manager, tom.wilson@company.com, 555-876-5432    Tom Wilson 35.0      Product Manager    tom.wilson@company.com 555-876-5432\n",
            "         8              lisa garcia, 29, UX Designer, lisa@design.com, (555) 345-6789   Lisa Garcia 29.0          UX Designer           lisa@design.com 555-345-6789\n"
          ]
        }
      ],
      "source": [
        "def batch_clean_entire_dataframe(df):\n",
        "    \"\"\"\n",
        "    Clean entire DataFrame in one batch request for consistency\n",
        "    \"\"\"\n",
        "    # Convert DataFrame to numbered list format\n",
        "    records_text = \"\\n\".join([\n",
        "        f\"{row['record_id']}. {row['raw_record']}\" \n",
        "        for _, row in df.iterrows()\n",
        "    ])\n",
        "    \n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a data cleaning expert who ensures consistency across all records in a dataset.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Clean this entire dataset while maintaining consistency across all records:\n",
        "\n",
        "Dataset:\n",
        "{records_text}\n",
        "\n",
        "Requirements:\n",
        "1. Standardize all names to Title Case\n",
        "2. Convert ages to numbers (use null for unknown/invalid)\n",
        "3. Standardize job titles consistently (e.g., 'Software Engineer' for all dev roles)\n",
        "4. Format all emails in lowercase\n",
        "5. Format all phones as XXX-XXX-XXXX\n",
        "6. Return as a JSON array with exactly {len(df)} objects\n",
        "\n",
        "Focus on consistency - if you see similar job titles, standardize them the same way.\n",
        "Return ONLY the JSON array:\"\"\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    response = call_llama(messages)\n",
        "    print(\"Batch Cleaning Response:\")\n",
        "    print(response[:500] + \"...\" if len(response) > 500 else response)\n",
        "    \n",
        "    # Try to parse the JSON array\n",
        "    try:\n",
        "        # Find the JSON array in the response\n",
        "        start_idx = response.find('[')\n",
        "        end_idx = response.rfind(']') + 1\n",
        "        \n",
        "        if start_idx != -1 and end_idx != 0:\n",
        "            json_str = response[start_idx:end_idx]\n",
        "            cleaned_data = json.loads(json_str)\n",
        "            cleaned_df = pd.DataFrame(cleaned_data)\n",
        "            \n",
        "            print(f\"\\nSuccessfully parsed {len(cleaned_df)} records\")\n",
        "            return cleaned_df\n",
        "        else:\n",
        "            print(\"\\nNo JSON array found in response\")\n",
        "            return None\n",
        "            \n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"\\nJSON parsing error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Clean the entire dataset\n",
        "batch_cleaned_df = batch_clean_entire_dataframe(messy_df)\n",
        "\n",
        "if batch_cleaned_df is not None:\n",
        "    print(\"\\nBatch Cleaned DataFrame:\")\n",
        "    print(batch_cleaned_df.to_string(index=False))\n",
        "    \n",
        "    # Combine with original data for comparison\n",
        "    comparison_df = pd.concat([\n",
        "        messy_df[['record_id', 'raw_record']].reset_index(drop=True),\n",
        "        batch_cleaned_df.reset_index(drop=True)\n",
        "    ], axis=1)\n",
        "    \n",
        "    print(\"\\nSide-by-side comparison:\")\n",
        "    print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Batch Processing Advantages and Challenges\n",
        "\n",
        "Batch processing represents a sophisticated approach to data cleaning:\n",
        "\n",
        "**Consistency Benefits**: \n",
        "- **Cross-record standardization**: The model sees all data simultaneously\n",
        "- **Pattern recognition**: Can identify and standardize similar job titles across records\n",
        "- **Global decision making**: Ensures consistent treatment of similar issues\n",
        "\n",
        "**Example Consistency Rules**: \n",
        "- **Job title standardization**: \"software engineer\" and \"developer\" → consistent choice\n",
        "- **Format uniformity**: All phone numbers follow the same pattern\n",
        "- **Case standardization**: All names use identical capitalization rules\n",
        "\n",
        "**Technical Challenges**: \n",
        "- **JSON array parsing**: More complex than single-record JSON\n",
        "- **Token limits**: Large datasets may exceed model context windows\n",
        "- **Error recovery**: One parsing error can affect the entire batch\n",
        "\n",
        "**Performance Considerations**: \n",
        "- **API efficiency**: One call vs. multiple calls (cost and speed)\n",
        "- **Memory usage**: Large DataFrames require more memory\n",
        "- **Error handling**: Balance between robustness and processing speed\n",
        "\n",
        "**When to Use Batch Processing**: \n",
        "- **Small to medium datasets**: Under token limits\n",
        "- **Consistency requirements**: When cross-record standardization matters\n",
        "- **Cost optimization**: When API call minimization is important\n",
        "\n",
        "### Data Quality Assessment: Measuring Success\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA QUALITY ASSESSMENT ===\n",
            "\n",
            "Original records: 8\n",
            "Cleaned records: 8\n",
            "\n",
            "Missing values in cleaned data:\n",
            "  name: 0 (0.0%)\n",
            "  age: 3 (37.5%)\n",
            "  jobTitle: 0 (0.0%)\n",
            "  email: 0 (0.0%)\n",
            "  phone: 0 (0.0%)\n",
            "\n",
            "Data types:\n",
            "name         object\n",
            "age         float64\n",
            "jobTitle     object\n",
            "email        object\n",
            "phone        object\n",
            "dtype: object\n",
            "\n",
            "Age statistics:\n",
            "  Valid ages: 5 / 8\n",
            "  Age range: 25.0 - 45.0\n",
            "  Average age: 32.4\n",
            "\n",
            "Email validation:\n",
            "  Valid email format: 8 / 8\n",
            "\n",
            "Phone validation:\n",
            "  Standard format (XXX-XXX-XXXX): 8 / 8\n",
            "\n",
            "Sample cleaned records:\n",
            "       name  age             jobTitle                 email        phone\n",
            "   John Doe 25.0    Software Engineer    john.doe@email.com 555-123-4567\n",
            " Jane Smith  NaN    Marketing Manager       jane@company.co 555-987-6543\n",
            "Bob Johnson  NaN Sales Representative bob.johnson@gmail.com 555-321-9876\n"
          ]
        }
      ],
      "source": [
        "def assess_data_quality(original_df, cleaned_df):\n",
        "    \"\"\"\n",
        "    Perform data quality assessment on cleaned DataFrame\n",
        "    \"\"\"\n",
        "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"\\nOriginal records: {len(original_df)}\")\n",
        "    print(f\"Cleaned records: {len(cleaned_df) if cleaned_df is not None else 0}\")\n",
        "    \n",
        "    if cleaned_df is not None:\n",
        "        # Check for missing values\n",
        "        print(\"\\nMissing values in cleaned data:\")\n",
        "        missing_counts = cleaned_df.isnull().sum()\n",
        "        for column, count in missing_counts.items():\n",
        "            percentage = (count / len(cleaned_df)) * 100\n",
        "            print(f\"  {column}: {count} ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Data type analysis\n",
        "        print(\"\\nData types:\")\n",
        "        print(cleaned_df.dtypes)\n",
        "        \n",
        "        # Age validation\n",
        "        if 'age' in cleaned_df.columns:\n",
        "            valid_ages = cleaned_df['age'].dropna()\n",
        "            if len(valid_ages) > 0:\n",
        "                print(f\"\\nAge statistics:\")\n",
        "                print(f\"  Valid ages: {len(valid_ages)} / {len(cleaned_df)}\")\n",
        "                print(f\"  Age range: {valid_ages.min()} - {valid_ages.max()}\")\n",
        "                print(f\"  Average age: {valid_ages.mean():.1f}\")\n",
        "        \n",
        "        # Email validation\n",
        "        if 'email' in cleaned_df.columns:\n",
        "            emails = cleaned_df['email'].dropna()\n",
        "            valid_emails = emails[emails.str.contains('@', na=False)]\n",
        "            print(f\"\\nEmail validation:\")\n",
        "            print(f\"  Valid email format: {len(valid_emails)} / {len(emails)}\")\n",
        "        \n",
        "        # Phone validation\n",
        "        if 'phone' in cleaned_df.columns:\n",
        "            phones = cleaned_df['phone'].dropna()\n",
        "            # Check for XXX-XXX-XXXX format\n",
        "            standard_format = phones.str.match(r'^\\d{3}-\\d{3}-\\d{4}$', na=False)\n",
        "            print(f\"\\nPhone validation:\")\n",
        "            print(f\"  Standard format (XXX-XXX-XXXX): {standard_format.sum()} / {len(phones)}\")\n",
        "        \n",
        "        # Show sample of cleaned data\n",
        "        print(\"\\nSample cleaned records:\")\n",
        "        print(cleaned_df.head(3).to_string(index=False))\n",
        "        \n",
        "        return cleaned_df\n",
        "    else:\n",
        "        print(\"\\nNo cleaned data available for assessment\")\n",
        "        return None\n",
        "\n",
        "# Assess the quality of our batch cleaned data\n",
        "quality_result = assess_data_quality(messy_df, batch_cleaned_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Comprehensive Data Quality Assessment\n",
        "\n",
        "Quality assessment is crucial for validating prompt engineering effectiveness:\n",
        "\n",
        "**Multi-Dimensional Quality Metrics**: \n",
        "\n",
        "1. **Completeness Assessment**: \n",
        "   - **Missing value analysis**: Identifies fields with incomplete data\n",
        "   - **Percentage calculations**: Quantifies data completeness\n",
        "   - **Field-specific analysis**: Different fields may have different completeness expectations\n",
        "\n",
        "2. **Data Type Validation**: \n",
        "   - **Type consistency**: Ensures fields contain expected data types\n",
        "   - **Format compliance**: Validates against business rules\n",
        "   - **Range validation**: Checks for reasonable value ranges\n",
        "\n",
        "3. **Domain-Specific Validation**: \n",
        "   - **Age reasonableness**: 18-100 range for employee data\n",
        "   - **Email format validation**: Ensures @ symbol presence\n",
        "   - **Phone format compliance**: XXX-XXX-XXXX pattern matching\n",
        "\n",
        "**Business Value of Quality Assessment**: \n",
        "- **Confidence building**: Quantifies cleaning effectiveness\n",
        "- **Process improvement**: Identifies areas needing prompt refinement\n",
        "- **Compliance support**: Documents data quality for regulatory requirements\n",
        "- **Decision support**: Provides metrics for choosing between cleaning approaches\n",
        "\n",
        "**Quality Metrics Interpretation**: \n",
        "- **100% email format compliance**: Indicates successful email standardization\n",
        "- **95%+ phone format compliance**: Shows effective pattern recognition\n",
        "- **Age statistics**: Validates reasonable value ranges and distributions\n",
        "\n",
        "### Production Data Cleaning Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PRODUCTION DATA CLEANING PIPELINE ===\n",
            "Processing record 1...\n",
            "Processing record 2...\n",
            "Processing record 3...\n",
            "Processing record 4...\n",
            "\n",
            "=== PROCESSING REPORT ===\n",
            "Processing status summary:\n",
            "  success: 4 (100.0%)\n",
            "\n",
            "Sample successful records (4 total):\n",
            "       name  age            job_title                 email        phone\n",
            "   John Doe 25.0    Software Engineer    john.doe@email.com 555-123-4567\n",
            " Jane Smith  NaN    Marketing Manager       jane@company.co 555-987-6543\n",
            "Bob Johnson 32.0 Sales Representative bob.johnson@gmail.com 555-321-9876\n",
            "\n",
            "Results saved to: /tmp/cleaned_data_results.csv\n"
          ]
        }
      ],
      "source": [
        "def production_data_cleaning_pipeline(df):\n",
        "    \"\"\"\n",
        "    Production-ready data cleaning pipeline with error handling\n",
        "    \"\"\"\n",
        "    print(\"=== PRODUCTION DATA CLEANING PIPELINE ===\")\n",
        "    \n",
        "    # Initialize results DataFrame\n",
        "    results_df = df.copy()\n",
        "    \n",
        "    # Add processing metadata columns\n",
        "    results_df['processing_status'] = 'pending'\n",
        "    results_df['processing_timestamp'] = pd.Timestamp.now()\n",
        "    results_df['cleaning_method'] = 'llm_prompt'\n",
        "    \n",
        "    cleaned_records = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            print(f\"Processing record {row['record_id']}...\")\n",
        "            \n",
        "            # Use role-based cleaning for production\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a data quality specialist. Clean data according to enterprise standards and return only valid JSON.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"Clean this record to enterprise standards. Return only JSON with fields: name, age, job_title, email, phone.\n",
        "\n",
        "Raw: \"{row['raw_record']}\"\n",
        "\n",
        "Standards:\n",
        "- Name: Title Case\n",
        "- Age: Integer 18-100 or null\n",
        "- Job Title: Standardized corporate titles\n",
        "- Email: Valid lowercase format\n",
        "- Phone: XXX-XXX-XXXX format\n",
        "\n",
        "JSON only:\"\"\"\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            response = call_llama(messages)\n",
        "            parsed_json = parse_cleaned_json(response)\n",
        "            \n",
        "            if parsed_json:\n",
        "                # Add metadata\n",
        "                parsed_json['source_record_id'] = row['record_id']\n",
        "                parsed_json['processing_status'] = 'success'\n",
        "                cleaned_records.append(parsed_json)\n",
        "                \n",
        "                # Update status in results\n",
        "                results_df.loc[idx, 'processing_status'] = 'success'\n",
        "                \n",
        "            else:\n",
        "                # Handle parsing failures\n",
        "                fallback_record = {\n",
        "                    'name': None,\n",
        "                    'age': None,\n",
        "                    'job_title': None,\n",
        "                    'email': None,\n",
        "                    'phone': None,\n",
        "                    'source_record_id': row['record_id'],\n",
        "                    'processing_status': 'failed_parsing',\n",
        "                    'error_message': 'Could not parse LLM response'\n",
        "                }\n",
        "                cleaned_records.append(fallback_record)\n",
        "                results_df.loc[idx, 'processing_status'] = 'failed_parsing'\n",
        "                \n",
        "        except Exception as e:\n",
        "            # Handle API or other errors\n",
        "            error_record = {\n",
        "                'name': None,\n",
        "                'age': None,\n",
        "                'job_title': None,\n",
        "                'email': None,\n",
        "                'phone': None,\n",
        "                'source_record_id': row['record_id'],\n",
        "                'processing_status': 'error',\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "            cleaned_records.append(error_record)\n",
        "            results_df.loc[idx, 'processing_status'] = 'error'\n",
        "            print(f\"Error processing record {row['record_id']}: {e}\")\n",
        "    \n",
        "    # Create cleaned DataFrame\n",
        "    cleaned_df = pd.DataFrame(cleaned_records)\n",
        "    \n",
        "    # Generate processing report\n",
        "    print(\"\\n=== PROCESSING REPORT ===\")\n",
        "    status_counts = results_df['processing_status'].value_counts()\n",
        "    print(\"Processing status summary:\")\n",
        "    for status, count in status_counts.items():\n",
        "        percentage = (count / len(results_df)) * 100\n",
        "        print(f\"  {status}: {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Show successful records\n",
        "    successful_records = cleaned_df[cleaned_df['processing_status'] == 'success']\n",
        "    if len(successful_records) > 0:\n",
        "        print(f\"\\nSample successful records ({len(successful_records)} total):\")\n",
        "        display_cols = ['name', 'age', 'job_title', 'email', 'phone']\n",
        "        print(successful_records[display_cols].head(3).to_string(index=False))\n",
        "    \n",
        "    # Save results to CSV (in real production, you'd save to database)\n",
        "    output_file = '/tmp/cleaned_data_results.csv'\n",
        "    cleaned_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nResults saved to: {output_file}\")\n",
        "    \n",
        "    return cleaned_df, results_df\n",
        "\n",
        "# Run production pipeline\n",
        "production_cleaned_df, production_results_df = production_data_cleaning_pipeline(messy_df.head(4))  # Process first 4 for demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Production-Ready Data Processing\n",
        "\n",
        "This production pipeline demonstrates enterprise-grade data cleaning:\n",
        "\n",
        "**Comprehensive Error Handling**: \n",
        "- **Try-catch blocks**: Prevent individual record failures from stopping the process\n",
        "- **Multiple error types**: Distinguishes between API errors and parsing failures\n",
        "- **Graceful degradation**: Continues processing even when some records fail\n",
        "\n",
        "**Metadata Tracking**: \n",
        "- **Processing timestamps**: Records when each operation occurred\n",
        "- **Status tracking**: Success, failure, parsing error categories\n",
        "- **Method documentation**: Records which cleaning approach was used\n",
        "- **Source traceability**: Links cleaned records back to originals\n",
        "\n",
        "**Business Process Integration**: \n",
        "- **Progress reporting**: Shows processing status for operational visibility\n",
        "- **Quality metrics**: Provides success/failure rates for process monitoring\n",
        "- **Output persistence**: Saves results for downstream processing\n",
        "- **Audit trail**: Maintains processing history for compliance\n",
        "\n",
        "**Enterprise Considerations**: \n",
        "- **Scalability**: Framework handles varying dataset sizes\n",
        "- **Reliability**: Robust error handling prevents data loss\n",
        "- **Monitoring**: Status tracking enables operational oversight\n",
        "- **Integration**: CSV output integrates with existing data pipelines\n",
        "\n",
        "**Role-Based Cleaning**: \n",
        "Uses data quality specialist role for:\n",
        "- **Enterprise standards**: Applies organizational data governance rules\n",
        "- **Professional expertise**: Leverages domain knowledge for cleaning decisions\n",
        "- **Consistent methodology**: Ensures uniform approach across all records\n",
        "\n",
        "### Approach Comparison: Choosing the Right Technique\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COMPARING APPROACHES FOR RECORD 2 ===\n",
            "Raw data: JANE SMITH,, marketing manager, jane@company.co, (555) 987-6543\n",
            "\n",
            "--- Zero-shot Approach ---\n",
            "Result: {\n",
            "  \"name\": \"Jane Smith\",\n",
            "  \"age\": null,\n",
            "  \"job_title\": \"Marketing Manager\",\n",
            "  \"email\": \"jane@company.co\",\n",
            "  \"phone\": \"555-987-6543\"\n",
            "}\n",
            "\n",
            "--- Role-based Approach ---\n",
            "Result: {\n",
            "  \"name\": \"JANE SMITH\",\n",
            "  \"email\": \"jane@company.co\",\n",
            "  \"phone\": \"(555) 987-6543\",\n",
            "  \"title\": \"marketing manager\"\n",
            "}\n",
            "\n",
            "=== COMPARISON SUMMARY ===\n",
            "  approach       name  age         job_title           email          phone             title\n",
            " Zero-shot Jane Smith  NaN Marketing Manager jane@company.co   555-987-6543               NaN\n",
            "Role-based JANE SMITH  NaN               NaN jane@company.co (555) 987-6543 marketing manager\n"
          ]
        }
      ],
      "source": [
        "def compare_cleaning_approaches(df, sample_record_idx=0):\n",
        "    \"\"\"\n",
        "    Compare different prompt engineering approaches on the same record\n",
        "    \"\"\"\n",
        "    target_record = df.iloc[sample_record_idx]\n",
        "    \n",
        "    print(f\"=== COMPARING APPROACHES FOR RECORD {target_record['record_id']} ===\")\n",
        "    print(f\"Raw data: {target_record['raw_record']}\")\n",
        "    \n",
        "    approaches = {\n",
        "        'Zero-shot': zero_shot_data_cleaning_single,\n",
        "        'Role-based': lambda record: call_llama([\n",
        "            {\"role\": \"system\", \"content\": \"You are a data cleaning expert.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Clean this record and return only JSON: {record}\"}\n",
        "        ])\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for approach_name, cleaning_func in approaches.items():\n",
        "        print(f\"\\n--- {approach_name} Approach ---\")\n",
        "        try:\n",
        "            response = cleaning_func(target_record['raw_record'])\n",
        "            parsed_json = parse_cleaned_json(response)\n",
        "            \n",
        "            if parsed_json:\n",
        "                results[approach_name] = parsed_json\n",
        "                print(\"Result:\", json.dumps(parsed_json, indent=2))\n",
        "            else:\n",
        "                results[approach_name] = {'error': 'Failed to parse'}\n",
        "                print(\"Failed to parse response\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            results[approach_name] = {'error': str(e)}\n",
        "            print(f\"Error: {e}\")\n",
        "    \n",
        "    # Create comparison DataFrame\n",
        "    comparison_data = []\n",
        "    for approach, result in results.items():\n",
        "        row = {'approach': approach}\n",
        "        row.update(result)\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"\\n=== COMPARISON SUMMARY ===\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Compare approaches on one record\n",
        "approach_comparison = compare_cleaning_approaches(messy_df, sample_record_idx=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Strategic Approach Selection\n",
        "\n",
        "This comparison framework helps you choose the optimal prompt engineering technique:\n",
        "\n",
        "**Approach Evaluation Criteria**: \n",
        "\n",
        "1. **Output Quality**: \n",
        "   - **Accuracy**: How correctly does each approach clean the data?\n",
        "   - **Consistency**: How uniform are the results across similar inputs?\n",
        "   - **Completeness**: How well does each approach handle missing data?\n",
        "\n",
        "2. **Processing Characteristics**: \n",
        "   - **Speed**: How quickly does each approach process data?\n",
        "   - **Cost**: What are the API usage costs for each method?\n",
        "   - **Scalability**: How well does each approach handle large datasets?\n",
        "\n",
        "3. **Business Fit**: \n",
        "   - **Transparency needs**: How important is understanding the cleaning process?\n",
        "   - **Consistency requirements**: How critical is cross-record standardization?\n",
        "   - **Error tolerance**: How much variation in output is acceptable?\n",
        "\n",
        "**Decision Framework**: \n",
        "\n",
        "- **Use Zero-Shot When**: \n",
        "  - Simple, well-defined cleaning rules\n",
        "  - Quick prototyping or one-off tasks\n",
        "  - Limited time for prompt development\n",
        "\n",
        "- **Use Few-Shot When**: \n",
        "  - Need consistent output formatting\n",
        "  - Have specific business rules to enforce\n",
        "  - Working with domain-specific data\n",
        "\n",
        "- **Use Chain of Thought When**: \n",
        "  - Complex cleaning decisions required\n",
        "  - Need to understand/debug the cleaning process\n",
        "  - Working in regulated environments\n",
        "\n",
        "- **Use Role-Based When**: \n",
        "  - Need domain expertise applied\n",
        "  - Want professional-grade output\n",
        "  - Have specific organizational standards\n",
        "\n",
        "- **Use Batch Processing When**: \n",
        "  - Cross-record consistency is critical\n",
        "  - Cost optimization is important\n",
        "  - Dataset size is manageable\n",
        "\n",
        "**Hybrid Approaches**: \n",
        "In practice, you might combine techniques:\n",
        "- **Few-shot + Role prompting**: Examples with expert persona\n",
        "- **CoT + Batch processing**: Reasoning with consistency\n",
        "- **Zero-shot + Production pipeline**: Simple rules with robust error handling\n",
        "\n",
        "---\n",
        "\n",
        "## Workshop Summary and Next Steps\n",
        "\n",
        "Congratulations! You've completed a comprehensive journey through the essential techniques of prompt engineering. Let's consolidate your learning and chart the path forward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Key Takeaways and Strategic Insights\n",
        "\n",
        "**Fundamental Principles You've Learned**: \n",
        "\n",
        "1. **Prompt Engineering is Problem-Solving**: \n",
        "   Each technique addresses specific challenges. Understanding when and why to use each approach is more valuable than memorizing prompt formats.\n",
        "\n",
        "2. **Context and Clarity Drive Success**: \n",
        "   The most effective prompts provide clear context, specific instructions, and well-defined expectations. Ambiguity is the enemy of consistent AI performance.\n",
        "\n",
        "3. **Examples Are Powerful Teachers**: \n",
        "   Few-shot prompting often provides the best balance of effort and results. Good examples can transform unreliable outputs into consistent, professional-grade results.\n",
        "\n",
        "4. **Reasoning Improves Complex Tasks**: \n",
        "   Chain of thought prompting breaks down complex problems and makes AI decision-making transparent and verifiable.\n",
        "\n",
        "5. **Roles Shape Behavior**: \n",
        "   Assigning appropriate roles to AI models can dramatically improve response quality by activating relevant knowledge and communication styles.\n",
        "\n",
        "**Business Applications and ROI**: \n",
        "\n",
        "The data cleaning examples demonstrate immediate business value:\n",
        "- **Time savings**: Automate hours of manual data standardization\n",
        "- **Cost reduction**: Reduce the need for specialized data cleaning tools or services\n",
        "- **Quality improvement**: Achieve more consistent results than manual processes\n",
        "- **Scalability**: Handle larger datasets with the same level of quality\n",
        "\n",
        "**Implementation Roadmap**: \n",
        "\n",
        "1. **Start Simple**: Begin with zero-shot prompting for straightforward tasks\n",
        "2. **Add Examples**: Develop few-shot prompts for tasks requiring consistency\n",
        "3. **Incorporate Reasoning**: Use CoT for complex decision-making processes\n",
        "4. **Develop Roles**: Create specialized personas for domain-specific tasks\n",
        "5. **Build Pipelines**: Integrate prompt engineering into production workflows\n",
        "\n",
        "**Advanced Topics for Further Exploration**: \n",
        "\n",
        "- **Prompt Chaining**: Connecting multiple prompts for complex workflows\n",
        "- **Dynamic Prompting**: Adapting prompts based on input characteristics\n",
        "- **Evaluation Frameworks**: Systematically measuring prompt effectiveness\n",
        "- **Model Comparison**: Evaluating different models for specific tasks\n",
        "- **Prompt Optimization**: Iterative improvement of prompt performance\n",
        "\n",
        "**Building Your Prompt Engineering Practice**: \n",
        "\n",
        "1. **Create a Prompt Library**: Build reusable prompts for common tasks\n",
        "2. **Establish Quality Metrics**: Define success criteria for different use cases\n",
        "3. **Document Best Practices**: Record what works well in your domain\n",
        "4. **Iterate and Improve**: Continuously refine prompts based on results\n",
        "5. **Share and Collaborate**: Build organizational knowledge around effective prompting\n",
        "\n",
        "**Final Thoughts**: \n",
        "\n",
        "Prompt engineering is both an art and a science. While this workshop provides structured techniques and best practices, the most effective prompt engineers develop intuition through experimentation and practice. The key is to start with these proven techniques and adapt them to your specific needs, datasets, and business requirements.\n",
        "\n",
        "Remember that prompt engineering is rapidly evolving. Stay curious, keep experimenting, and don't hesitate to combine techniques in creative ways. The examples in this workshop are starting points—your unique applications and innovations will drive the most value for your organization.\n",
        "\n",
        "**Your Next Steps**: \n",
        "1. Choose a real dataset from your work and apply these techniques\n",
        "2. Compare the effectiveness of different approaches on your data\n",
        "3. Build a simple production pipeline using the patterns demonstrated\n",
        "4. Share your results and learnings with colleagues\n",
        "5. Continue exploring advanced prompt engineering techniques"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
